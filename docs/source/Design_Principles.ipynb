{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caefc7fb",
   "metadata": {},
   "source": [
    "# Design principles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a87d1be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import feature_encoders.settings\n",
    "\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ab1284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eensight.settings\n",
    "\n",
    "from eensight.config import OmegaConfigLoader\n",
    "from eensight.utils.jupyter import load_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc2bb92",
   "metadata": {},
   "source": [
    "## Data Pipelines\n",
    "\n",
    "All the functionality in `eensight` is organized around data pipelines. Each pipeline consumes data and other artifacts (such as models) produced by a previous pipeline, and produces new data and artifacts for its successor pipelines. Up to this point, there are six (6) pipelines:\n",
    "\n",
    "| Pipeline name \t| Pipeline description                                                                                                                                                                                                                                                                              \t|\n",
    "|:---------------\t|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\t|\n",
    "| *preprocess*    \t| Merges (if necessary) and validates input data, identifies potential data drift, identifies potential outliers, evaluates data adequacy, and imputes missing values.                                                                                                                                              \t|\n",
    "| *daytype*       \t| Finds consumption profile prototypes and estimates a distance metric that translates calendar information (month of year and day of week) to daily or sub-daily consumption profile similarity. Prototypes are a small set of daily or sub-daily profiles that adequately summarize the available data.              \t|\n",
    "| *baseline*      \t| Optimizes a predictive model for out-of-sample performance, fits the optimized model on the available training data, and evaluates its performance in-sample. \t|\n",
    "| *validate*      \t| Cross-validates the optimized predictive model and builds a conformal predictor to construct uncertainty intervals. \t|\n",
    "| *predict*       \t|     Uses the optimized predictive model and the conformal model of the previous stage to generate predictions on pre- and post-retrofit data, adding uncertainty intervals with user-provided confidence levels.                                                                                                               \t|\n",
    "| *compare*       \t| Estimates cumulative savings given the optimized predictive model and post-retrofit data, while adding uncertainty intervals with user-provided confidence levels.                                                                                                                                 \t|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f8405c",
   "metadata": {},
   "source": [
    "Each pipeline has up to two (2) different versions, each corresponding to one of the two (2) **namespaces** in `eensight`:\n",
    "\n",
    "* **train**: Any dataset or pipeline that exists in the `train` namespace will be used for model training and evaluation. \n",
    "\n",
    "\n",
    "* **apply**: Any dataset or pipeline that exists in the `apply` namespace will be used for counterfactual prediction and savings estimation. \n",
    "\n",
    "The associations between pipelines and namespaces are summarized below:\n",
    "\n",
    "|            \t| train    \t| apply   \t| \n",
    "|------------\t|----------\t|----------\t|\n",
    "| preprocess \t| &#10004; \t| &#10004; \t| \n",
    "| daytype    \t| &#10004; \t|          \t|          \t\n",
    "| baseline   \t| &#10004; \t|  \t|  \n",
    "| validate    \t| &#10004; \t| \t|\n",
    "| predict    \t| &#10004; \t| &#10004;\t| \n",
    "| compare    \t|          \t| &#10004;  | \n",
    "\n",
    "\n",
    "`eensight` is a [Kedro](https://github.com/quantumblacklabs/kedro)-based application, so all pipelines are [Kedro pipelines](https://kedro.readthedocs.io/en/stable/06_nodes_and_pipelines/02_pipeline_introduction.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742a8a83",
   "metadata": {},
   "source": [
    "## Counterfactual prediction\n",
    "\n",
    "When the forecast counterfactual prediction approach is selected, a predictive model is trained on the data before the energy efficiency intervention, and applied on the data after the intervention:\n",
    "\n",
    "<img src=\"images/forward.png\" alt=\"catalogs\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28706be2",
   "metadata": {},
   "source": [
    "For the `eensight` user, the application of the forecast approach requires only the assignment of the available datasets to the correct namespaces. The forecast approach requires that the `train` namespace corresponds to data before the energy efficiency intervention and the `apply` namespace corresponds to data after the energy efficiency intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48fa366",
   "metadata": {},
   "source": [
    "## Data catalogs\n",
    "\n",
    "All pipelines get and store their datasets, parameters and models by interacting with a [data catalog](https://kedro.readthedocs.io/en/stable/05_data/01_data_catalog.html). However, `eensight` extends and customizes the relevant functionality of Kedro so that multiple catalogs can coexist. While Kedro assumes a unique catalog for any Kedro project, `eensight` allows users to define a new catalog for each building site. The goal is to support the very common use case where M&V practitioners have access to a set of CSV or Excel files for each building they want to analyze. As a result, all `eensight` commands need the name of the catalog with which they will interact.\n",
    "\n",
    "<img src=\"images/catalogs.png\" alt=\"catalogs\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1090441f",
   "metadata": {},
   "source": [
    "A data catalog is a YAML file that includes: \n",
    "\n",
    "- **The name of the building/site**\n",
    "\n",
    "    Example:\n",
    "    ```yaml\n",
    "    site_name : demo\n",
    "    ```\n",
    "\n",
    "    `eensight` assumes that the data is located at a relative path that is composed by the site name, the name of the folder according to the Kedro's [data engineering convention](https://kedro.readthedocs.io/en/stable/12_faq/01_faq.html#what-is-data-engineering-convention), and the corresponding namespace. The root of the relative path is defined in `eensight.settings.DATA_ROOT`.\n",
    "    \n",
    "    The root of the relative path is defined in: `eensight.settings.DATA_ROOT`.\n",
    "\n",
    "\n",
    "- **Whether or not datasets and models should be versioned**. In order to enable [versioning](https://kedro.readthedocs.io/en/stable/05_data/02_kedro_io.html#versioning), you need to update the catalog file and set the versioned attribute to true.   \n",
    "\n",
    "    Example:\n",
    "    ```yaml\n",
    "    versioned : true\n",
    "    ```\n",
    "\n",
    "\n",
    "- **The location of the building**. This information is used for automatically generating holiday information, if the selected model requires a \"holiday\" feature.\n",
    "\n",
    "    Example:\n",
    "    ```yaml\n",
    "    location:\n",
    "      country   : Greece  \n",
    "      province  : null \n",
    "      state     : null \n",
    "    ```\n",
    "\n",
    "- **A map of feature names**. This is a mapping between specific feature names that the `eensight` functionality expects (consumption, temperature, holiday, timestamp) and the actual names used in the available datasets\n",
    "\n",
    "    Example:\n",
    "    ```yaml\n",
    "    rebind_names:\n",
    "      consumption : eload\n",
    "      temperature : temp \n",
    "      holiday     : null \n",
    "      timestamp   : dates\n",
    "    ```\n",
    "\n",
    "- The different data sources consumed and produced by `eensight`. The information that is necessary to adequately describe a data source can be found at Kedro's documentation: https://kedro.readthedocs.io/en/stable/05_data/01_data_catalog.html    \n",
    "\n",
    "  Adding the appropriate namespace at the beginning of a dataset's name allows us to automate the process of selecting and running pipelines:\n",
    "  \n",
    "  ```yaml\n",
    "    train.root_input:\n",
    "      type: PartitionedDataSet\n",
    "      path: demo/01_raw/train\n",
    "      dataset:\n",
    "        type: pandas.CSVDataSet\n",
    "        load_args:\n",
    "          sep: ','\n",
    "          index_col: 0\n",
    "      filename_suffix: '.csv'\n",
    "\n",
    "    test.root_input:\n",
    "      type: PartitionedDataSet\n",
    "      path: demo/01_raw/test\n",
    "      dataset:\n",
    "        type: pandas.CSVDataSet\n",
    "        load_args:\n",
    "          sep: ','\n",
    "          index_col: 0\n",
    "      filename_suffix: '.csv'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9533d1e8",
   "metadata": {},
   "source": [
    "One of the changes that `eensight` has introduced to the Kedro's standard approach is a custom `ConfigLoader` (`eensight.config.OmegaConfigLoader`) that utilizes [OmegaConf](https://github.com/omry/omegaconf) as backend. `OmegaConf` makes it easy to use [variable interpolation](https://omegaconf.readthedocs.io/en/latest/usage.html#variable-interpolation) when writting the configuration files. As an example, the `globals.yaml` file contains values that can be reused in other files:  \n",
    "\n",
    "*globals.yaml:*\n",
    "```yaml\n",
    "types:\n",
    "  csv      : pandas.CSVDataSet\n",
    "  json     : json.JSONDataSet\n",
    "  multiple : PartitionedDataSet\n",
    "  pickle   : pickle.PickleDataSet\n",
    "\n",
    "folders:\n",
    "  raw          : 01_raw\n",
    "  intermediate : 02_intermediate\n",
    "  primary      : 03_primary\n",
    "  feature      : 04_feature\n",
    "  model_input  : 05_model_input\n",
    "  model        : 06_models\n",
    "  model_output : 07_model_output\n",
    "  report       : 08_reporting\n",
    "```\n",
    "\n",
    "... and then in *some_catalog.yaml:*\n",
    "```yaml\n",
    "site_name : demo\n",
    "\n",
    "train.root_input:\n",
    "    type: ${globals:types.multiple}\n",
    "    path: ${site_name}/${globals:folders.raw}/train\n",
    "    dataset:\n",
    "      type: ${globals:types.csv}\n",
    "      load_args:\n",
    "        sep: ','\n",
    "        index_col: 0\n",
    "      filename_suffix: '.csv'\n",
    "```\n",
    "\n",
    "It is not necessary to use variable interpolation in `eensight`'s configuration files, but it simplifies the process of writting them. One could have also defined *some_catalog.yaml* as: \n",
    "\n",
    "```yaml\n",
    "site_name : demo\n",
    "\n",
    "train.root_input:\n",
    "    type: PartitionedDataSet\n",
    "    path: demo/01_raw/train\n",
    "    dataset:\n",
    "      type: pandas.CSVDataSet\n",
    "      load_args:\n",
    "        sep: ','\n",
    "        index_col: 0\n",
    "    filename_suffix: '.csv'\n",
    "```\n",
    "\n",
    "Variable interpolation makes it also easy to define **partial catalogs**. Since the largest part of a catalog declares intermediate and final artifacts, users may write only the part of the catalog that corresponds to the raw input data:\n",
    "\n",
    "```yaml\n",
    "site_name : demo\n",
    "versioned : false   \n",
    "\n",
    "train.root_input:\n",
    "  type: PartitionedDataSet\n",
    "  path: demo/01_raw/train\n",
    "  dataset:\n",
    "    type: pandas.CSVDataSet\n",
    "    load_args:\n",
    "      sep: ','\n",
    "      index_col: 0\n",
    "  filename_suffix: '.csv'\n",
    "\n",
    "test.root_input:\n",
    "  type: PartitionedDataSet\n",
    "  path: demo/01_raw/test\n",
    "  dataset:\n",
    "    type: pandas.CSVDataSet\n",
    "    load_args:\n",
    "      sep: ','\n",
    "      index_col: 0\n",
    "  filename_suffix: '.csv'\n",
    "```\n",
    "\n",
    "> **Note**: The `site_name` and `versioned` entries are **required**, whereas `location` and `rebind_names` not.\n",
    "\n",
    "All the commands or methods that have the name of the catalog as an input, have also the option to indicate whether it is partial or complete. If the catalog is partial, it is merged with the template from `eensight.settings.CONF_ROOT/base/templates/_base.yaml` and interpolated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d829528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join(eensight.settings.CONF_ROOT, \"base\")\n",
    "\n",
    "if not os.path.isabs(base_path):\n",
    "    base_path = os.path.join(eensight.settings.PROJECT_PATH, base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e265e",
   "metadata": {},
   "source": [
    "The demo catalog is a partial catalog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87eef03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"site_name\": \"demo\",\n",
      "    \"versioned\": false,\n",
      "    \"location\": {\n",
      "        \"country\": \"Italy\",\n",
      "        \"province\": null,\n",
      "        \"state\": null\n",
      "    },\n",
      "    \"rebind_names\": {\n",
      "        \"consumption\": null,\n",
      "        \"temperature\": null,\n",
      "        \"holiday\": null,\n",
      "        \"timestamp\": null\n",
      "    },\n",
      "    \"train.root_input\": {\n",
      "        \"type\": \"PartitionedDataSet\",\n",
      "        \"path\": \"demo/01_raw/train\",\n",
      "        \"dataset\": {\n",
      "            \"type\": \"pandas.CSVDataSet\",\n",
      "            \"load_args\": {\n",
      "                \"sep\": \",\",\n",
      "                \"index_col\": 0\n",
      "            }\n",
      "        },\n",
      "        \"filename_suffix\": \".csv\"\n",
      "    },\n",
      "    \"apply.root_input\": {\n",
      "        \"type\": \"PartitionedDataSet\",\n",
      "        \"path\": \"${site_name}/01_raw/apply\",\n",
      "        \"dataset\": {\n",
      "            \"type\": \"pandas.CSVDataSet\",\n",
      "            \"load_args\": {\n",
      "                \"sep\": \",\",\n",
      "                \"index_col\": 0\n",
      "            }\n",
      "        },\n",
      "        \"filename_suffix\": \".csv\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "catalog_conf = OmegaConf.load(os.path.join(base_path, \"catalogs\", \"demo\", \"catalog.yaml\"))\n",
    "print(json.dumps(OmegaConf.to_container(catalog_conf), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06bcbbe",
   "metadata": {},
   "source": [
    "The `eensight.framework.context.CustomContext` completes a partial catalog using an approach that is similar to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff55cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"train.model_prediction\": {\n",
      "        \"type\": \"pandas.CSVDataSet\",\n",
      "        \"filepath\": \"demo/07_model_output/train/model_prediction.csv\",\n",
      "        \"load_args\": {\n",
      "            \"sep\": \",\",\n",
      "            \"index_col\": 0,\n",
      "            \"parse_dates\": [\n",
      "                0\n",
      "            ]\n",
      "        },\n",
      "        \"save_args\": {\n",
      "            \"index\": true\n",
      "        },\n",
      "        \"versioned\": false\n",
      "    },\n",
      "    \"apply.model_prediction\": {\n",
      "        \"type\": \"pandas.CSVDataSet\",\n",
      "        \"filepath\": \"demo/07_model_output/apply/model_prediction.csv\",\n",
      "        \"load_args\": {\n",
      "            \"sep\": \",\",\n",
      "            \"index_col\": 0,\n",
      "            \"parse_dates\": [\n",
      "                0\n",
      "            ]\n",
      "        },\n",
      "        \"save_args\": {\n",
      "            \"index\": true\n",
      "        },\n",
      "        \"versioned\": false\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config_loader = OmegaConfigLoader(conf_paths=[base_path], globals_pattern=\"globals*\")\n",
    "\n",
    "selected_catalog = \"demo\"\n",
    "catalog_is_partial = True\n",
    "\n",
    "catalog_search = [\n",
    "    f\"catalogs/{selected_catalog}.*\",\n",
    "    f\"catalogs/{selected_catalog}/**\",\n",
    "    f\"catalogs/**/{selected_catalog}.*\",\n",
    "]\n",
    "\n",
    "if catalog_is_partial:\n",
    "    catalog_search.append(\"templates/_base.*\")\n",
    "\n",
    "conf_catalog = config_loader.get(*catalog_search)\n",
    "print(json.dumps(\n",
    "            {\n",
    "                key: val for key, val in conf_catalog.items() \n",
    "                if \"model_prediction\" in key\n",
    "            },\n",
    "         indent=4\n",
    "      )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c3e13e",
   "metadata": {},
   "source": [
    "Another way to create a data catalog is by using the command: `python -m eensight catalog create`. This command will guide the user through a series of questions. This is a simple approach that is meant to work for input data in the form of csv files. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e2c24",
   "metadata": {},
   "source": [
    "## Energy consumption baseline modelling approach\n",
    "\n",
    "\n",
    "Global non-linear models – such as Random Forest Trees, Gradient Boosting Trees or Neural Nets – can be very effective in terms of predicting building energy consumption. This is evident when one looks at the winning solutions of the [ASHRAE’s Great Energy Predictor III competition](https://www.kaggle.com/c/ashrae-energy-prediction) hosted by Kaggle; the top five (5) approaches utilized combinations of Gradient Boosting Trees (LightGBM, CatBoost, XGBoost) and Neural Nets (feed-forward networks and convolutional networks). \n",
    "\n",
    "However, interpretation and auditing of a global non-linear model is not trivial. Here, model interpretation is related to the transparency of an algorithm’s decisions, and the ability to identify what the algorithm has learned and what subset of the observations was most influential on what the algorithm learned.   \n",
    "\n",
    "An alternative to global non-linear models is an ensemble of local linear models. The general recipe that `eensight` uses for developing an ensemble of such models comprises the following steps:\n",
    "\n",
    " 1.\tSelect the linear base model to be used as the local estimator (i.e. the building block of the ensemble);\n",
    " 2.\tDefine a way to quantify the notion of locality, i.e. when two (2) observations are close enough to be handled by the same local model;\n",
    " 3. Find a rule to assign all observations into neighborhoods in feature space;\n",
    " 4. Fit one model per neighborhood;\n",
    " 5. Predict using the average of all the models' predictions. If a test data point is outside the neighborhood of a model, that model does not contribute to the prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fdba2c",
   "metadata": {},
   "source": [
    "## Base model configuration\n",
    "\n",
    "The structure of the linear base model can be defined using YAML files. These files have three sections: (a) added features, (b) regressors and (c) interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890364ad",
   "metadata": {},
   "source": [
    "### 1. Added features\n",
    "\n",
    "The information in this section is passed to one of the feature generators. Most of the feature generators are provided by the [`feature-encoders` library](https://github.com/hebes-io/feature-encoders). There is extensive [documentation](https://feature-encoders.readthedocs.io/en/latest/Tutorial_Feature_Composition.html) on how feature generators are defined in `feature-encoders`, and how additional feature generators can be developed.  \n",
    "\n",
    "As an example, to include a [`feature_encoders.generate.DatetimeFeatures`](https://feature-encoders.readthedocs.io/en/latest/feature_encoders.generate.html#feature_encoders.generate.DatetimeFeatures) generator in a model, one would have to add in the model YAML specification:\n",
    "\n",
    "```yaml\n",
    "add_features:\n",
    "  time:\n",
    "    type: datetime\n",
    "    subset: month, hourofweek\n",
    "```\n",
    "\n",
    "`eensight` gets the information on how to map feature generator types to the classes for input validation and creation of the corresponding generators using an approach similar to the one below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d912942",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_path = feature_encoders.settings.CONF_ROOT\n",
    "\n",
    "if not os.path.isabs(feature_path):\n",
    "    feature_path = os.path.join(\n",
    "        feature_encoders.settings.PROJECT_PATH, feature_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "949889b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_loader = OmegaConfigLoader(conf_paths=[feature_path, base_path], \n",
    "                                  globals_pattern=\"globals*\"\n",
    ")\n",
    "\n",
    "conf_features = config_loader.get(\n",
    "    \"features*\", \"features/**\", \"**/features*\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c377373",
   "metadata": {},
   "source": [
    "`TrendFeatures`, `DatetimeFeatures` and `CyclicalFeatures` generators are provided by `feature-encoders` (no need for a fully qualified class name), while `HolidayFeatures` and `OccupancyFeatures` are provided by `eensight.features` (fully qualified class names must be provided). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a9cc021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"trend\": {\n",
      "        \"validate\": \"validate.TrendSchema\",\n",
      "        \"generate\": \"generate.TrendFeatures\"\n",
      "    },\n",
      "    \"datetime\": {\n",
      "        \"validate\": \"validate.DatetimeSchema\",\n",
      "        \"generate\": \"generate.DatetimeFeatures\"\n",
      "    },\n",
      "    \"cyclical\": {\n",
      "        \"validate\": \"validate.CyclicalSchema\",\n",
      "        \"generate\": \"generate.CyclicalFeatures\"\n",
      "    },\n",
      "    \"holidays\": {\n",
      "        \"validate\": \"eensight.features.HolidaySchema\",\n",
      "        \"generate\": \"eensight.features.HolidayFeatures\"\n",
      "    },\n",
      "    \"occupancy\": {\n",
      "        \"validate\": \"eensight.features.OccupancySchema\",\n",
      "        \"generate\": \"eensight.features.OccupancyFeatures\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(conf_features, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f3fc2",
   "metadata": {},
   "source": [
    "From all the parameters under the name of a feature generator in a catalog configuration, \n",
    "\n",
    "```yaml\n",
    "add_features:\n",
    "  time:\n",
    "    type: datetime\n",
    "    subset: month, hourofweek\n",
    "```\n",
    "\n",
    "the `type` is used to identify the correct classes for validation and creation. The remaining are passed to the validation class and then to the `__init__` method of the creation class.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca82baa8",
   "metadata": {},
   "source": [
    "### 2. Regressors\n",
    "\n",
    "The information for each regressor includes its name, the name of the feature to encode so that to create this regressor, the type of the encoder (linear, spline or categorical), and the parameters to pass to the corresponding encoder class from `feature-encoders`:\n",
    "\n",
    "- `IdentityEncoder` for type linear \n",
    "- `SplineEncoder` for type spline\n",
    "- `CategoricalEncoder` for type categorical\n",
    "\n",
    " \n",
    "Example:\n",
    "\n",
    "```yaml\n",
    "regressors:\n",
    "  month:              # name of the regressor\n",
    "    feature: month    # name of the feature \n",
    "    type: categorical # type of the encoder to apply on `feature` and create `regressor`\n",
    "    encode_as: onehot # parameter for encoder's __init__\n",
    "    \n",
    "  tow:                     # name of the regressor\n",
    "    feature: hourofweek    # name of the feature \n",
    "    type: categorical\n",
    "    max_n_categories: 60   # lump together 168 categories into 60 by target similarity\n",
    "    encode_as: onehot \n",
    "    \n",
    "  flex_temperature:\n",
    "    feature: temperature\n",
    "    type: spline\n",
    "    n_knots: 5\n",
    "    degree: 1\n",
    "    strategy: uniform \n",
    "    extrapolation: constant\n",
    "    interaction_only: true # do not include it in main effects\n",
    "\n",
    "```\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73850c4",
   "metadata": {},
   "source": [
    "### 3. Interactions\n",
    "\n",
    "Interactions can introduce new regressors, reuse regressors already defined in the regressors section, as well as alter the parameters of regressors that are already defined in the regressors section. Interactions are always pairwise and always between encoders (and not features).\n",
    "\n",
    "This functionality too is provided by [`feature-encoders`](https://feature-encoders.readthedocs.io/en/latest/Tutorial_Interactions.html).\n",
    "\n",
    "```yaml\n",
    "interactions:\n",
    "  tow, flex_temperature:\n",
    "    tow: # update the parameters of the tow encoder\n",
    "      max_n_categories: 2 \n",
    "      stratify_by: temperature \n",
    "      min_samples_leaf: 15 \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a6bc42",
   "metadata": {},
   "source": [
    "`eensight` gets the information on model configurations using an approach similar to the one below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd754413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"add_features\": {\n",
      "        \"time\": {\n",
      "            \"type\": \"datetime\",\n",
      "            \"subset\": \"month, hourofweek\"\n",
      "        }\n",
      "    },\n",
      "    \"regressors\": {\n",
      "        \"month\": {\n",
      "            \"feature\": \"month\",\n",
      "            \"type\": \"categorical\",\n",
      "            \"encode_as\": \"onehot\"\n",
      "        },\n",
      "        \"tow\": {\n",
      "            \"feature\": \"hourofweek\",\n",
      "            \"type\": \"categorical\",\n",
      "            \"encode_as\": \"onehot\"\n",
      "        },\n",
      "        \"lin_temperature\": {\n",
      "            \"feature\": \"temperature\",\n",
      "            \"type\": \"linear\"\n",
      "        },\n",
      "        \"flex_temperature\": {\n",
      "            \"feature\": \"temperature\",\n",
      "            \"type\": \"spline\",\n",
      "            \"n_knots\": 5,\n",
      "            \"degree\": 1,\n",
      "            \"strategy\": \"uniform\",\n",
      "            \"extrapolation\": \"constant\",\n",
      "            \"interaction_only\": true\n",
      "        }\n",
      "    },\n",
      "    \"interactions\": {\n",
      "        \"tow, flex_temperature\": {\n",
      "            \"tow\": {\n",
      "                \"max_n_categories\": 2,\n",
      "                \"stratify_by\": \"temperature\",\n",
      "                \"min_samples_leaf\": 15\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config_loader = OmegaConfigLoader(conf_paths=[feature_path, base_path], \n",
    "                                  globals_pattern=\"globals*\"\n",
    ")\n",
    "\n",
    "selected_model = \"towt\"\n",
    "\n",
    "conf_model = config_loader.get(\n",
    "    f\"base_models/{selected_model}.*\",\n",
    "    f\"base_models/{selected_model}/**\",\n",
    "    f\"**/base_models/{selected_model}.*\",\n",
    ")\n",
    "\n",
    "print(json.dumps(conf_model, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28642913",
   "metadata": {},
   "source": [
    "## Parameter values\n",
    "\n",
    "`eensight` pipelines get their parameter settings from YAML files in the *conf/base/parameters* directory.\n",
    "\n",
    "```\n",
    "conf\n",
    "│   README.md \n",
    "│\n",
    "└───base\n",
    "│   │   globals.yaml\n",
    "│   │   logging.yaml\n",
    "│   │\n",
    "│   └── parameters\n",
    "│       ├── preprocess.yaml\n",
    "│       ├── ...\n",
    "   \n",
    "```\n",
    "\n",
    "Parameters are accessed and treated exactly as prescribed by the Kedro documentation: https://kedro.readthedocs.io/en/stable/04_kedro_project_setup/02_configuration.html#parameters\n",
    "\n",
    "`eensight` gets the information for parameters using an approach similar to the one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c789bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'of_optimize_model',\n",
      "  'of_apply_compare',\n",
      "  'of_prepare_data',\n",
      "  'of_find_prototypes',\n",
      "  'of_distance_learning',\n",
      "  'daytyping_window',\n",
      "  'of_apply_predict',\n",
      "  'of_drift_detection',\n",
      "  'find_outliers_for',\n",
      "  'of_global_filter',\n",
      "  'of_seasonal_decompose',\n",
      "  'of_global_outlier',\n",
      "  'of_local_outlier',\n",
      "  'of_linear_impute',\n",
      "  'max_missing_pct',\n",
      "  'max_outlier_pct',\n",
      "  'of_cross_validate',\n",
      "  'of_conformal_predictor']\n"
     ]
    }
   ],
   "source": [
    "config_loader = OmegaConfigLoader(conf_paths=[feature_path, base_path], \n",
    "                                  globals_pattern=\"globals*\"\n",
    ")\n",
    "\n",
    "# we merge all YAML files under conf/base/parameters\n",
    "params = config_loader.get(\n",
    "    \"parameters*\", \"parameters*/**\", \"**/parameters*\"\n",
    ")\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "pp.pprint(list(params.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5152007d",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "`load_catalog` is a utility function that loads a data catalog in the same way that `eensight.framework.context.CustomContext` does it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ead6c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = load_catalog(\"demo\", partial_catalog=True, base_model=\"towt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b843082f",
   "metadata": {},
   "source": [
    "The catalog includes data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fff9799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.model_prediction', 'apply.model_prediction']\n"
     ]
    }
   ],
   "source": [
    "pp.pprint([item for item in catalog.list() if \"model_prediction\" in item])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80434940",
   "metadata": {},
   "source": [
    "... information about the feature generators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9137c9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-21 16:23:46,077 - kedro.io.data_catalog - INFO - Loading data from `feature_map` (MemoryDataSet)...\n",
      "{\n",
      "    \"trend\": {\n",
      "        \"validate\": \"validate.TrendSchema\",\n",
      "        \"generate\": \"generate.TrendFeatures\"\n",
      "    },\n",
      "    \"datetime\": {\n",
      "        \"validate\": \"validate.DatetimeSchema\",\n",
      "        \"generate\": \"generate.DatetimeFeatures\"\n",
      "    },\n",
      "    \"cyclical\": {\n",
      "        \"validate\": \"validate.CyclicalSchema\",\n",
      "        \"generate\": \"generate.CyclicalFeatures\"\n",
      "    },\n",
      "    \"holidays\": {\n",
      "        \"validate\": \"eensight.features.HolidaySchema\",\n",
      "        \"generate\": \"eensight.features.HolidayFeatures\"\n",
      "    },\n",
      "    \"occupancy\": {\n",
      "        \"validate\": \"eensight.features.OccupancySchema\",\n",
      "        \"generate\": \"eensight.features.OccupancyFeatures\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(catalog.load(\"feature_map\"), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276dcf04",
   "metadata": {},
   "source": [
    "... the selected model configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cf53445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-21 16:23:48,959 - kedro.io.data_catalog - INFO - Loading data from `model_config` (MemoryDataSet)...\n",
      "{\n",
      "    \"add_features\": {\n",
      "        \"time\": {\n",
      "            \"type\": \"datetime\",\n",
      "            \"subset\": \"month, hourofweek\"\n",
      "        }\n",
      "    },\n",
      "    \"regressors\": {\n",
      "        \"month\": {\n",
      "            \"feature\": \"month\",\n",
      "            \"type\": \"categorical\",\n",
      "            \"encode_as\": \"onehot\"\n",
      "        },\n",
      "        \"tow\": {\n",
      "            \"feature\": \"hourofweek\",\n",
      "            \"type\": \"categorical\",\n",
      "            \"encode_as\": \"onehot\"\n",
      "        },\n",
      "        \"lin_temperature\": {\n",
      "            \"feature\": \"temperature\",\n",
      "            \"type\": \"linear\"\n",
      "        },\n",
      "        \"flex_temperature\": {\n",
      "            \"feature\": \"temperature\",\n",
      "            \"type\": \"spline\",\n",
      "            \"n_knots\": 5,\n",
      "            \"degree\": 1,\n",
      "            \"strategy\": \"uniform\",\n",
      "            \"extrapolation\": \"constant\",\n",
      "            \"interaction_only\": true\n",
      "        }\n",
      "    },\n",
      "    \"interactions\": {\n",
      "        \"tow, flex_temperature\": {\n",
      "            \"tow\": {\n",
      "                \"max_n_categories\": 2,\n",
      "                \"stratify_by\": \"temperature\",\n",
      "                \"min_samples_leaf\": 15\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(catalog.load(\"model_config\"), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bbe784",
   "metadata": {},
   "source": [
    "... and all the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63fd7c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-21 16:23:54,022 - kedro.io.data_catalog - INFO - Loading data from `parameters` (MemoryDataSet)...\n",
      "[ 'of_optimize_model',\n",
      "  'of_apply_compare',\n",
      "  'of_prepare_data',\n",
      "  'of_find_prototypes',\n",
      "  'of_distance_learning',\n",
      "  'daytyping_window',\n",
      "  'of_apply_predict',\n",
      "  'of_drift_detection',\n",
      "  'find_outliers_for',\n",
      "  'of_global_filter',\n",
      "  'of_seasonal_decompose',\n",
      "  'of_global_outlier',\n",
      "  'of_local_outlier',\n",
      "  'of_linear_impute',\n",
      "  'max_missing_pct',\n",
      "  'max_outlier_pct',\n",
      "  'of_cross_validate',\n",
      "  'of_conformal_predictor']\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(list(catalog.load(\"parameters\").keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4e054",
   "metadata": {},
   "source": [
    "## Run command arguments\n",
    "\n",
    "The primary way of using `eensight` is through the command line: <code>python -m eensight run </code>\n",
    "\n",
    "It is [Click](https://click.palletsprojects.com/)-based, so -h and --help commands can provide information about the expected usage.  \n",
    "\n",
    "| Option \t| Description \t|\n",
    "|:---\t|:---\t|\n",
    "| catalog \t| The name of the catalog configuration file to load.  \t|\n",
    "| partial-catalog \t| Whether the catalog includes information only about the raw input data |\n",
    "| base-model \t| The name of the base model configuration file to load \t|\n",
    "| pipeline \t| The name of the modular pipeline to run \t|\n",
    "| runner \t| The runner that you want to run the pipeline with \t|\n",
    "| parallel \t| Flag to run the pipeline using the `ParallelRunner` \t|\n",
    "| async \t| If load and save node inputs and outputs should be done asynchronously with threads \t|\n",
    "| env \t| Kedro configuration environment name \t|\n",
    "| from-inputs \t| A list of dataset names which should be used as a starting point \t|\n",
    "| to-outputs \t| A list of dataset names which should be used as an end point \t|\n",
    "| from-nodes \t| A list of node (pipeline step) names which should be used as a starting point \t|\n",
    "| to-nodes \t| A list of node names which should be used as an end point \t|\n",
    "| nodes \t| A list with node names. The `run` function will run only these nodes \t|\n",
    "| tags \t| List of tags. Construct a pipeline from nodes having any of these tags \t|\n",
    "| load-versions \t| A mapping between dataset names and versions to load \t|\n",
    "| params \t| These values will override the values in the `parameters` configuration files |\n",
    "| run-config| A YAML configuration file to load the missing (if any) `run` command arguments from. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b04574b",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
