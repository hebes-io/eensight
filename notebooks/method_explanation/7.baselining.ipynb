{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "746ca838",
   "metadata": {},
   "source": [
    "# The baselining pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0f6fc8",
   "metadata": {},
   "source": [
    "Global non-linear models – such as Random Forest Trees, Gradient Boosting Trees or Neural Nets – can be very effective in terms of predicting building energy consumption. This is evident when one looks at the winning solutions of the [ASHRAE’s Great Energy Predictor III competition](https://www.kaggle.com/c/ashrae-energy-prediction) hosted by Kaggle; the top five (5) approaches utilized combinations of Gradient Boosting Trees (LightGBM, CatBoost, XGBoost) and Neural Nets (feed-forward networks and convolutional networks). \n",
    "\n",
    "However, interpretation and auditing of a global non-linear model is not trivial. Here, model interpretation is related to the transparency of an algorithm’s decisions, and the ability to identify what the algorithm has learned and what subset of the observations was most influential on what the algorithm learned.   \n",
    "\n",
    "An alternative to global non-linear models is an ensemble of local linear models. The general recipe for developing an ensemble of such models comprises the following steps:\n",
    "\n",
    " 1.\tSelect the linear model to be used as the local estimator (i.e. the building block of the ensemble);\n",
    " 2.\tDefine a way to quantify the notion of locality, i.e. when two (2) observations are close enough to be handled by the same local model;\n",
    " 3.\tDefine a strategy for selecting the observations to train each local model and for combining the results for all individual models into one prediction.   \n",
    "\n",
    "The way `eensight` implements these steps is explained here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3398d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "526a810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from joblib import Parallel\n",
    "from sklearn.base import clone\n",
    "from datetime import time, datetime\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils.fixes import delayed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "0f7f70e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eensight.utils.jupyter import load_catalog\n",
    "\n",
    "from eensight.features.generate import DatetimeFeatures, MMCFeatures\n",
    "from eensight.features.encode import CategoricalEncoder, SplineEncoder, ICatSplineEncoder\n",
    "from eensight.pipelines.model_selection import cvrmse, nmbe\n",
    "from eensight.features.cluster import ClusterFeatures\n",
    "from eensight.models import (\n",
    "    LinearPredictor,\n",
    "    GroupedPredictor,\n",
    "    CompositePredictor,\n",
    "    EnsemblePredictor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7709475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = load_catalog('demo')\n",
    "\n",
    "data_train = catalog.load('train.model_input_data')\n",
    "\n",
    "# We train the predictive models without outliers:\n",
    "X_train = data_train.loc[~data_train['consumption_outlier'], ['temperature']]\n",
    "y_train = data_train.loc[X_train.index, ['consumption']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d047c663",
   "metadata": {},
   "source": [
    "## The linear model used as the local estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad79698",
   "metadata": {},
   "source": [
    "The simplest model to use is a model that includes only the hour of the week as a feature. The hour of the week is a categorical feature and it can be encoded in [one-hot form](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-categorical-features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "17bf99f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = DatetimeFeatures(subset='hourofweek').fit_transform(X_train)[['hourofweek']]\n",
    "dmatrix = CategoricalEncoder(feature='hourofweek', encode_as='onehot').fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10711ba",
   "metadata": {},
   "source": [
    "We can fit a linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "77456e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=False).fit(dmatrix, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016f6faf",
   "metadata": {},
   "source": [
    "... and evaluate it in-sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8e96eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(dmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb0fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_train.values\n",
    "\n",
    "print(f\"In-sample CV(RMSE) (%): {cvrmse(y_true, pred)*100}\")\n",
    "print(f\"In-sample NMBE (%): {nmbe(y_true, pred)*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c47fcf",
   "metadata": {},
   "source": [
    "The degrees of freedom of the model are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8aeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(dmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43bdd0b",
   "metadata": {},
   "source": [
    "The impact of the hour of the week on energy consumption is then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d296a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(data=pred, index=y_train.index, columns=['hourofweek_impact'])\n",
    "\n",
    "date_enc = DatetimeFeatures(remainder='passthrough', subset='hourofweek')\n",
    "to_plot = date_enc.fit_transform(pred).groupby('hourofweek').mean()\n",
    "\n",
    "\n",
    "colors = ['#8c510a', '#d8b365', '#f6e8c3', '#f5f5f5', '#c7eae5', '#5ab4ac', '#01665e']\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):    \n",
    "    fig = plt.figure(figsize=(12, 3), dpi=96)\n",
    "    layout = (1, 1)\n",
    "    ax = plt.subplot2grid(layout, (0, 0))\n",
    "\n",
    "    intervals = np.split(to_plot.index, 7)\n",
    "    for i, item in enumerate(intervals):\n",
    "        ax.axvspan(item[0], item[-1], alpha=0.3, color=colors[i])\n",
    "    \n",
    "    to_plot.plot(ax=ax)\n",
    "    ax.set_xlabel('Hour of week')\n",
    "    ax.legend(['Average contribution of hour-of-week feature'], fancybox=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e957a14e",
   "metadata": {},
   "source": [
    "`eensight` includes functionality for reducing the number of categories in a categorical feature while retaining as much as possible the feature's predictive capability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d6c3b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = DatetimeFeatures(subset='hourofweek').fit_transform(X_train)[['hourofweek']]\n",
    "\n",
    "enc = CategoricalEncoder(feature='hourofweek', encode_as='onehot', max_n_categories=60)\n",
    "dmatrix = enc.fit_transform(features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7839bbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=False).fit(dmatrix, y_train.values)\n",
    "pred = model.predict(dmatrix)\n",
    "\n",
    "print(f\"In-sample CV(RMSE) (%): {cvrmse(y_true, pred)*100}\")\n",
    "print(f\"In-sample NMBE (%): {nmbe(y_true, pred)*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b975f3e",
   "metadata": {},
   "source": [
    "This is practically the same performance with one third of degrees of freedom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d12e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(dmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b11a72",
   "metadata": {},
   "source": [
    "The local model uses this functionality so that to control the degrees of freedom of the ensemble model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf4930f",
   "metadata": {},
   "source": [
    "Another component to include in the model is an interaction between the hour of the week and the temperature. The [TOWT model](https://ieeexplore.ieee.org/document/5772947/) estimates the temperature effect separately for periods of the day with high and with low energy consumption in order to distinguish between occupied and unoccupied building periods. \n",
    "\n",
    "To this end, a flexible curve is fitted on the `consumption~temperature` relationship, and if more than the 65% of the data points that correspond to a specific time-of-week are above the fitted curve, the corresponding hour is flagged as “Occupied”, otherwise it is flagged as “Unoccupied.” \n",
    "\n",
    "We can apply this approach using `eensight` functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a281141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = SplineEncoder(feature='temperature', degree=1, strategy='uniform').fit(X_train)\n",
    "dmatrix = enc.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "221c0402",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=False).fit(dmatrix, y_train.values)\n",
    "pred = model.predict(dmatrix)\n",
    "pred = pd.DataFrame(data=pred, index=y_train.index, columns=y_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0a167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):    \n",
    "    fig = plt.figure(figsize=(12, 3), dpi=96)\n",
    "    layout = (1, 1)\n",
    "    ax = plt.subplot2grid(layout, (0, 0))\n",
    "    \n",
    "    ax.scatter(X_train['temperature'], y_train['consumption'], s=1, alpha=0.2)\n",
    "    \n",
    "    X_train_ = X_train.sort_values(by='temperature')\n",
    "    ax.plot(X_train_, pred.loc[X_train_.index, 'consumption'], c='#cc4c02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "047baebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = y_train - pred\n",
    "mask = resid > 0\n",
    "mask = DatetimeFeatures(subset='hourofweek').fit_transform(mask)\n",
    "occupied = mask.groupby('hourofweek')['consumption'].mean() > 0.65\n",
    "occupied = occupied.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e2905f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = DatetimeFeatures(subset='hourofweek').fit_transform(X_train)\n",
    "features['occupied'] = features['hourofweek'].map(lambda x: occupied[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "012d3c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_temp = SplineEncoder(feature='temperature', degree=1)\n",
    "enc_occ = CategoricalEncoder(feature='occupied', encode_as='onehot')\n",
    "enc_occ = enc_occ.fit(features)\n",
    "\n",
    "enc = ICatSplineEncoder(encoder_cat=enc_occ, encoder_num=enc_temp)\n",
    "dmatrix = enc.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76baa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=False).fit(dmatrix, y_train.values)\n",
    "pred = model.predict(dmatrix)\n",
    "\n",
    "print(f\"In-sample CV(RMSE) (%): {cvrmse(y_true, pred)*100}\")\n",
    "print(f\"In-sample NMBE (%): {nmbe(y_true, pred)*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cc4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(dmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4640eaf",
   "metadata": {},
   "source": [
    "Alternatively, we can rely on `eensight`s functionality to categorize the hours of the week into the two most dissimilar categories in terms of energy consumption given temperature information: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e0f02d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = DatetimeFeatures(subset='hourofweek').fit_transform(X_train)\n",
    "\n",
    "enc_temp = SplineEncoder(feature='temperature', degree=1, strategy='uniform')\n",
    "enc_occ = CategoricalEncoder(feature='hourofweek', max_n_categories=2,\n",
    "                             stratify_by='temperature', min_samples_leaf=15)\n",
    "enc_occ = enc_occ.fit(features, y_train)\n",
    "\n",
    "enc = ICatSplineEncoder(encoder_cat=enc_occ, encoder_num=enc_temp)\n",
    "dmatrix = enc.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5300185",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=False).fit(dmatrix, y_train.values)\n",
    "pred = model.predict(dmatrix)\n",
    "\n",
    "print(f\"In-sample CV(RMSE) (%): {cvrmse(y_true, pred)*100}\")\n",
    "print(f\"In-sample NMBE (%): {nmbe(y_true, pred)*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070168ed",
   "metadata": {},
   "source": [
    "The prediction results are better while the number of the degrees of freedom is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461be581",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(dmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630245a9",
   "metadata": {},
   "source": [
    "Then, the `consumption~temperature` curves per category of hour of week are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a37742",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_enc = DatetimeFeatures(remainder='passthrough', subset='hourofweek')\n",
    "\n",
    "intervals = pd.concat(\n",
    "    ( pd.cut(X_train['temperature'], 15, precision=0), \n",
    "      pd.DataFrame(data=pred, index=X_train.index, columns=['temperature_impact'])\n",
    "    ), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "enc_cat = enc_occ.feature_pipeline_['reduce_dimension']\n",
    "intervals = date_enc.fit_transform(intervals)\n",
    "intervals['hourofweek'] = intervals['hourofweek'].map(lambda x: enc_cat.mapping_[x])\n",
    "\n",
    "to_plot = (\n",
    "    intervals.groupby(['hourofweek', 'temperature'])['temperature_impact']\n",
    "             .mean()\n",
    "             .unstack()\n",
    ")\n",
    "\n",
    "colors = ['#8c510a', '#df65b0']\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):    \n",
    "    fig = plt.figure(figsize=(12, 3), dpi=96)\n",
    "    layout = (1, 1)\n",
    "    ax = plt.subplot2grid(layout, (0, 0))\n",
    "    \n",
    "    for i, (idx, values) in enumerate(to_plot.iterrows()):\n",
    "        values.plot(ax=ax, lw=2, alpha=0.6, label=f'category {idx}', color=colors[i])\n",
    "\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "    ax.set_xlabel('Temperature intervals')\n",
    "    ax.legend(fancybox=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0589ccb7",
   "metadata": {},
   "source": [
    "The only two additions to the above components are:\n",
    "\n",
    "1. A categorical feature for the different months of the dataset\n",
    "2. A linear term for the temperature as a main effect. The interaction term between temperature and the hour of the week \"corrects\" the predictions of the temperature's linear term in the main effects.\n",
    "\n",
    "The default local linear model used in `eensight` is defined in *eensight/conf/base/models/towt.yaml* as:\n",
    "\n",
    "```yaml\n",
    "add_features:\n",
    "  time:\n",
    "    type: datetime\n",
    "    subset: month, hourofweek\n",
    "  \n",
    "regressors:\n",
    "  month:\n",
    "    feature: month\n",
    "    type: categorical\n",
    "    encode_as: onehot\n",
    "\n",
    "  tow:\n",
    "    feature: hourofweek\n",
    "    type: categorical\n",
    "    max_n_categories: 60  \n",
    "    encode_as: onehot\n",
    "  \n",
    "  lin_temperature:\n",
    "    feature: temperature\n",
    "    type: linear\n",
    "  \n",
    "  flex_temperature:\n",
    "    feature: temperature\n",
    "    type: spline\n",
    "    n_knots: 5\n",
    "    degree: 1\n",
    "    strategy: uniform \n",
    "    extrapolation: constant\n",
    "    interaction_only: true\n",
    "\n",
    "interactions:\n",
    "  tow, flex_temperature:\n",
    "    tow:\n",
    "      max_n_categories: 2 \n",
    "      stratify_by: temperature \n",
    "      min_samples_leaf: 15 \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78342a08",
   "metadata": {},
   "source": [
    "We can apply the local model on the whole demo dataset. This functionality is provided by `eensight.models.LinearPredictor`:\n",
    "\n",
    "### `eensight.models.LinearPredictor` \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_structure : dict\n",
    "        The model configuration\n",
    "    alpha : float (default=0.1)\n",
    "        Regularization strength of the underlying ridge regression; must be a positive \n",
    "        float. Regularization improves the conditioning of the problem and reduces the \n",
    "        variance of the estimates. Larger values specify stronger regularization.\n",
    "    fit_intercept : bool (default=False)\n",
    "        Whether to fit the intercept for this model. If set to false, no intercept will \n",
    "        be used in calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "353d3321",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = load_catalog('demo', model='towt')\n",
    "model_structure = catalog.load('model_structure')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1ed90e",
   "metadata": {},
   "source": [
    "Create the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f7bfd2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_reg = LinearPredictor(model_structure=model_structure, alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359adb06",
   "metadata": {},
   "source": [
    "Fit with training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338bf35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "local_reg = local_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4948f39",
   "metadata": {},
   "source": [
    "Evaluate the model in-sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a50d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pred = local_reg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_train\n",
    "\n",
    "print(f\"In-sample CV(RMSE) (%): {cvrmse(y_true[['consumption']], pred[['consumption']])*100}\")\n",
    "print(f\"In-sample NMBE (%): {nmbe(y_true[['consumption']], pred[['consumption']])*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab2881",
   "metadata": {},
   "source": [
    "We can plot the standardized residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "1134363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = y_true[['consumption']] - pred[['consumption']]\n",
    "resid = StandardScaler().fit_transform(resid).squeeze()\n",
    "resid = pd.Series(data=resid, index=y_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129087cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    grid = sns.jointplot(x=pred['consumption'], y=resid)\n",
    "    grid.fig.set_figwidth(12)\n",
    "    grid.fig.set_figheight(5)\n",
    "    grid.set_axis_labels(xlabel='Predicted Value', ylabel='Standardized Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe60b55",
   "metadata": {},
   "source": [
    "The effective number of parameters (i.e. the degrees of freedom) is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44b41d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_reg.n_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e866d4be",
   "metadata": {},
   "source": [
    "This is how the design matrix of the regression corresponds to each regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ef7fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_reg.composer_.component_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cd7d3c",
   "metadata": {},
   "source": [
    "This makes it easy to decompose the prediction into components (the regularization term `alpha=0.01` in the `LinearPredictor` was used primarily so that the iindividual components have reasonable values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90153a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pred = local_reg.predict(X_train, include_components=True)\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "58b0ba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(pred['consumption'],\n",
    "            pred[[col for col in pred.columns if col != 'consumption']].sum(axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b625f5",
   "metadata": {},
   "source": [
    "## A notion of locality\n",
    "\n",
    "\n",
    "The local model is meant to be applied on different clusters of the dataset. Since each of the local models in the ensemble predicts on a different subset of the input data (an observation cannot belong to more than one clusters), the final prediction is generated by vertically concatenating all the individual models’ predictions.\n",
    " \n",
    "\n",
    "<img src=\"images/grouped.png?modified=12345678\" alt=\"grouped\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dffaf2a",
   "metadata": {},
   "source": [
    "This functionality is provided by the combination of the: \n",
    "\n",
    "- `eensight.features.ClusterFeatures`\n",
    "- `eensight.models.GroupedPredictor`\n",
    "\n",
    "\n",
    "### `eensight.features.ClusterFeatures` \n",
    "\n",
    "A composite transformer model that uses [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/) (Hierarchical Density-Based \n",
    "Spatial Clustering of Applications with Noise) to cluster the input data and \n",
    "a [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) to predict the clusters for unseen inputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    min_cluster_size : int, optional (default=5)\n",
    "        The minimum size of clusters; single linkage splits that contain\n",
    "        fewer points than this will be considered points \"falling out\" of a\n",
    "        cluster rather than a cluster splitting into two new clusters.\n",
    "    min_samples : int, optional (default=5)\n",
    "        The number of samples in a neighbourhood for a point to be\n",
    "        considered a core point. This parameter controls what the clusterer\n",
    "        identifies as noise.\n",
    "    metric : string, or callable, optional (default='euclidean')\n",
    "        The metric to use when calculating distance between instances in a\n",
    "        feature array. If metric is a string or callable, it must be one of\n",
    "        the options allowed by metrics.pairwise.pairwise_distances for its\n",
    "        metric parameter. If metric is \"precomputed\", X is assumed to be a\n",
    "        distance matrix and must be square.\n",
    "    transformer : An object that implements a `fit_transform` method (default=None)\n",
    "        The `fit_transform` method is used for transforming the input into a form\n",
    "        that is understood by the distance metric.\n",
    "    memory : Instance of joblib.Memory or string (optional)\n",
    "        Used to cache the output of the computation of the tree.\n",
    "        If a string is given, it is the path to the caching directory.\n",
    "    allow_single_cluster : bool, optional (default=True)\n",
    "        By default HDBSCAN will not produce a single cluster, setting this\n",
    "        to True will override this and allow single cluster results in\n",
    "        the case that you feel this is a valid result for your dataset.\n",
    "    cluster_selection_method : string, optional (default='eom')\n",
    "        The method used to select clusters from the condensed tree. The\n",
    "        standard approach for HDBSCAN* is to use an Excess of Mass algorithm\n",
    "        to find the most persistent clusters. Alternatively you can instead\n",
    "        select the clusters at the leaves of the tree -- this provides the\n",
    "        most fine grained and homogeneous clusters. Options are:\n",
    "            * ``eom``\n",
    "            * ``leaf``\n",
    "    n_neighbors : int, default=1\n",
    "        Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
    "    weights : {'uniform', 'distance'} or callable, default='uniform'\n",
    "        weight function used in prediction. Possible values:\n",
    "        - 'uniform' : uniform weights.  All points in each neighborhood\n",
    "        are weighted equally.\n",
    "        - 'distance' : weight points by the inverse of their distance.\n",
    "        In this case, closer neighbors of a query point will have a\n",
    "        greater influence than neighbors which are further away.\n",
    "        - [callable] : a user-defined function which accepts an\n",
    "        array of distances, and returns an array of the same shape\n",
    "        containing the weights.\n",
    "    output_name : str, default='cluster'\n",
    "        The name of the output dataframe's column that includes the cluster\n",
    "        information.\n",
    "\n",
    "\n",
    "\n",
    "### `eensight.models.GroupedPredictor` \n",
    "\n",
    "Constructs one estimator per data group. Splits data by values of a\n",
    "single column and fits one estimator per such column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_structure : dict\n",
    "        A configuration dictionary that includes information about the base model's\n",
    "        structure.\n",
    "    group_feature : str\n",
    "        The name of the column of the input dataframe to use as the grouping set.\n",
    "    estimator_params : dict or tuple of tuples, default=tuple()\n",
    "        The parameters to use when instantiating a new base estimator. If none are given,\n",
    "        default parameters are used.\n",
    "    fallback : bool (default=False)\n",
    "        Whether or not to fall back to a global model in case a group parameter is not\n",
    "        found during `.predict()`.\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28f67b4",
   "metadata": {},
   "source": [
    "We already have the `distance_metrics` in the catalog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a967af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_metrics = catalog.load('train.distance_metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a54ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distance_metrics.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8bab30",
   "metadata": {},
   "source": [
    "Select a time interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "7766ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time(8, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e6ca9c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = distance_metrics[start_time]['end_time']\n",
    "\n",
    "metric_components = distance_metrics[start_time]['metric_components']\n",
    "metric = functools.partial(metric_function, metric_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "35386c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_features = Pipeline([\n",
    "        ('dates', DatetimeFeatures(subset=['month', 'dayofweek'])),\n",
    "        ('features', MMCFeatures()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1af56cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = ClusterFeatures(min_cluster_size=20, \n",
    "                            transformer=mcc_features, \n",
    "                            output_name='cluster', \n",
    "                            metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "55678b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_int = X_train.loc[start_time:end_time]\n",
    "y_train_int = y_train.loc[start_time:end_time]\n",
    "\n",
    "clusters = clusterer.fit_transform(X_train_int)\n",
    "\n",
    "X_train_int = pd.concat((X_train_int, clusters), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "6f125fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_grouped = GroupedPredictor(\n",
    "                    model_structure=model_structure, \n",
    "                    group_feature='cluster',\n",
    "                    estimator_params=(('alpha', 0.01), ('fit_intercept', False)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a9a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reg_grouped = reg_grouped.fit(X_train_int, y_train_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59681da8",
   "metadata": {},
   "source": [
    "The `GroupedPredictor` applies the feature generation transformers directly on the dataset before it is split per cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b9c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_grouped.transformers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc19a0a6",
   "metadata": {},
   "source": [
    "In addition, it fits all categorical encoders in ordinal form, and passes the encoded data (but not the actual encoders) to each cluster estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8368585",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, encoder in reg_grouped.encoders_['main_effects'].items():\n",
    "    print('--->', name)\n",
    "    print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8bc2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair_name, encoder in reg_grouped.encoders_['interactions'].items():\n",
    "    print('--->', pair_name)\n",
    "    print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800553aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pred = reg_grouped.predict(X_train_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0ed0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_train_int\n",
    "\n",
    "print(f\"In-sample CV(RMSE) (%): {cvrmse(y_true[['consumption']], pred[['consumption']])*100}\")\n",
    "print(f\"In-sample NMBE (%): {nmbe(y_true[['consumption']], pred[['consumption']])*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb578ddb",
   "metadata": {},
   "source": [
    "The number of parameters was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b968c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_grouped.n_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d119af29",
   "metadata": {},
   "source": [
    "... and the model was fitted using observation data size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d31babd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_int.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b259d0",
   "metadata": {},
   "source": [
    "The process of combining distance metrics per time interval, clusters and base models is encapsulated into the `CompositePredictor`:\n",
    "\n",
    "\n",
    "### `eensight.models.CompositePredictor` \n",
    "\n",
    "Linear regression model that combines a clusterer (an estimator that answers \n",
    "the question \"*To which cluster should I allocate a given observation's target?*\")\n",
    "and a grouped regressor (regressor for predicting the target given information \n",
    "about the clusters)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    distance_metrics : dict\n",
    "        Dictionary containing time interval information of the form:\n",
    "        key: interval start time, values: interval end time, and components of \n",
    "        the corresponding distance metric.\n",
    "    base_clusterer : eensight.features.cluster.ClusterFeatures\n",
    "        An estimator that answers the question \"To which cluster should I allocate\n",
    "        a given observation's target?\".\n",
    "    base_regressor : eensight.models.grouped.GroupedPredictor\n",
    "        A regressor for predicting the target given information about the clusters.\n",
    "    group_feature : str, default='cluster'\n",
    "        The name of the feature to use as the grouping set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "19953b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CompositePredictor(distance_metrics=distance_metrics, \n",
    "                           base_clusterer=clusterer,\n",
    "                           base_regressor=reg_grouped\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9daaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b28e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062adb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_train\n",
    "\n",
    "print(f\"In-sample CV(RMSE) (%): {cvrmse(y_true[['consumption']], pred[['consumption']])*100}\")\n",
    "print(f\"In-sample NMBE (%): {nmbe(y_true[['consumption']], pred[['consumption']])*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a9c24b",
   "metadata": {},
   "source": [
    "The number of parameters used was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1aa60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.n_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7316f9",
   "metadata": {},
   "source": [
    "We can ask for the cluster information to be included in the prediction (cluster information is in the form `# of interval: # of cluster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc47d991",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pred = model.predict(X_train, include_clusters=True)\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff12526",
   "metadata": {},
   "source": [
    "... and/or the components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e0b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pred = model.predict(X_train, include_components=True)\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "37117c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(pred['consumption'],\n",
    "            pred[[col for col in pred.columns if col != 'consumption']].sum(axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be55ad4",
   "metadata": {},
   "source": [
    "The parameter `min_cluster_size` of the `ClusterFeatures` can (and generally should) be optimized for out of sample performance. \n",
    "\n",
    "The relevant functionality is provided by `eensight.pipelines.model_selection.optimize`:\n",
    "\n",
    "### `eensight.pipelines.model_selection.optimize` \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : Any regressor with scikit-learn API (i.e. with fit and\n",
    "        predict methods) \n",
    "        The object to use to fit the data.\n",
    "    X : pandas dataframe of shape (n_samples, n_features)\n",
    "        The input data to optimize on.\n",
    "    y : pandas dataframe of shape of shape (n_samples, 1)\n",
    "        The training target data to optimize on.\n",
    "    n_repeats : int, default=2\n",
    "        Number of times to repeat the train/test data split process process.\n",
    "    test_size : float, default=0.25\n",
    "        The proportion of the dataset to include in the test split. Should be between\n",
    "        0.0 and 1.0.\n",
    "    target_name : str, default='consumption'\n",
    "        It is expected that both y and the predictions of the `estimator` are\n",
    "        dataframes with a single column, the name of which is the one provided\n",
    "        for `target_name`.\n",
    "    budget : int, default=20\n",
    "        The number of trials. If this argument is set to `None`, there is no\n",
    "        limitation on the number of trials. If `timeout` is also set to `None`,\n",
    "        the study continues to create trials until it receives a termination\n",
    "        signal such as Ctrl+C or SIGTERM.\n",
    "    timeout : int, default=None\n",
    "        Stop study after the given number of second(s). If this argument is set to\n",
    "        `None`, the study is executed without time limitation.\n",
    "    scorers : dict, default=None\n",
    "        dict mapping scorer name to a callable. The callable object\n",
    "        should have signature ``scorer(y_true, y_pred)``.\n",
    "        The default value is:\n",
    "        `OrderedDict(\n",
    "            {\n",
    "                \"CVRMSE\": lambda y_true, y_pred:\n",
    "                    eensight.pipelines.model_selection.cvrmse(\n",
    "                        y_true[target_name], y_pred[target_name]\n",
    "                    ),\n",
    "                \"ExVAR\": lambda y_true, y_pred: sklearn.metrics.explained_variance_score(\n",
    "                    y_true[target_name], y_pred[target_name]\n",
    "                ),\n",
    "            }\n",
    "        )`\n",
    "    directions : list, default=None\n",
    "        A sequence of directions during multi-objective optimization. Set\n",
    "        ``minimize`` for minimization and ``maximize`` for maximization.\n",
    "        The default value is ['minimize', 'maximize'].\n",
    "    optimization_space : callable, default=None\n",
    "        A function that takes an `optuna.trial.Trial` as input and returns\n",
    "        a parameter combination to try. If it is None, the `estimator` should\n",
    "        have an `optimization_space` function.\n",
    "    multivariate: bool, default=False\n",
    "        If `True`, the multivariate TPE (Tree-structured Parzen Estimator)\n",
    "        is used when suggesting parameters.\n",
    "    out_of_sample: bool, default=True\n",
    "        Whether the optimization should be based on out-of-sample (if `True`) or\n",
    "        in-sample (if `False`) performance.\n",
    "    verbose : bool, default=False\n",
    "        Flag to show progress bars or not.\n",
    "    tags: str or list of str, default=None\n",
    "        Tags are returned by the function as-is and are useful as a way to distinguish\n",
    "        the results when running the function many times in parallel.\n",
    "    opt_space_kwards : dict\n",
    "        Additional keyworded parameters to pass to the `optimization_space`\n",
    "        function.\n",
    "\n",
    "The optimization can be carried out once with `cluster_selection_method='eom'` and once with `cluster_selection_method='leaf'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "31044c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_for_tag(model, X_train, y_train, budget, tag):\n",
    "    model.set_params(**{\"base_clusterer__assign_clusters__cluster_selection_method\": tag})\n",
    "    return optimize(model, X_train, y_train, budget=budget, \n",
    "                    tags=tag, multivariate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "176ea53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = ClusterFeatures(min_cluster_size=20, \n",
    "                            transformer=mcc_features, \n",
    "                            output_name='cluster')\n",
    "\n",
    "reg_grouped = GroupedPredictor(model_structure=model_structure, \n",
    "                               group_feature='cluster',\n",
    "                               estimator_params=(('alpha', 0.01), ('fit_intercept', False)),\n",
    ")\n",
    "\n",
    "model = CompositePredictor(distance_metrics=distance_metrics, \n",
    "                           base_clusterer=clusterer,\n",
    "                           base_regressor=reg_grouped,\n",
    "                           group_feature='cluster'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ce2c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "budget = 15\n",
    "\n",
    "parallel = Parallel(n_jobs=2)\n",
    "results = parallel(\n",
    "        delayed(optimize_for_tag)(\n",
    "                clone(model), \n",
    "                X_train, \n",
    "                y_train,\n",
    "                budget, \n",
    "                tag\n",
    "        ) for tag in ['eom', 'leaf']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e25c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = None\n",
    "params= None\n",
    "\n",
    "for res in results:\n",
    "    scores = pd.concat((scores, res['scores']), ignore_index=True)\n",
    "    params = pd.concat(\n",
    "    ( params,\n",
    "      res['params'].assign(**{'assign_clusters__cluster_selection_method': res['tags']})\n",
    "    ), ignore_index=True)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdecc823",
   "metadata": {},
   "source": [
    "We can consider all parameter combinations (they constitute a Pareto front) and create an ensemble out of them:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "d7e6e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_include = scores.sort_values(by='CVRMSE').iloc[:5]\n",
    "params_ = params.loc[to_include.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "84deda9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_model = EnsemblePredictor(base_estimator=model,\n",
    "                             ensemble_parameters=params_.to_dict('records')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c45b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "en_model = en_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d61111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pred = en_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08df650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_train\n",
    "\n",
    "print(f\"In-sample CV(RMSE) (%): {cvrmse(y_true[['consumption']], pred[['consumption']])*100}\")\n",
    "print(f\"In-sample NMBE (%): {nmbe(y_true[['consumption']], pred[['consumption']])*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2379ad",
   "metadata": {},
   "source": [
    "The number of parameters is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79228833",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_model.n_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba4a71",
   "metadata": {},
   "source": [
    "The individual components propagate from the local models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185188bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = en_model.predict(X_train, include_components=True)\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "3a32b08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(pred['consumption'],\n",
    "    pred[[col for col in pred.columns if col not in ('consumption', 'cluster')]].sum(axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96be6bb",
   "metadata": {},
   "source": [
    "We can plot the standardized residuals, adding this time information about the matrix profile scores: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "989f2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = y_true[['consumption']] - pred[['consumption']]\n",
    "resid = StandardScaler().fit_transform(resid).squeeze()\n",
    "resid = pd.Series(data=resid, index=y_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "0d0f2d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_scores = catalog.load('train.matrix_profile_scores')\n",
    "\n",
    "discords = mp_scores[mp_scores['nnd'] >= mp_scores['nnd'].quantile(0.99)]\n",
    "discords = data_train.loc[np.isin(data_train.index.date, discords.index.date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355be54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "discord_resids = resid.loc[discords.index]\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    grid = sns.jointplot(x=pred['consumption'], y=resid)\n",
    "    sns.scatterplot(x=pred.loc[discord_resids.index, 'consumption'], y=discord_resids, \n",
    "                    color='#feb24c', ax=grid.ax_joint)\n",
    "    grid.fig.set_figwidth(12)\n",
    "    grid.fig.set_figheight(5)\n",
    "    grid.set_axis_labels(xlabel='Predicted Value', ylabel='Standardized Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af72c337",
   "metadata": {},
   "source": [
    "This is the main value from identifying discords: discords that are not handled well by the model are data subsets for which getting additional information will be most beneficial. For the dataset at hand, getting this information is easy; all discords correspond to holidays: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c60561",
   "metadata": {},
   "outputs": [],
   "source": [
    "discords['holiday'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83e5967",
   "metadata": {},
   "source": [
    "`eensight` includes a model specification for datasets with holiday information at *eensight/conf/base/models/towt_holidays.yaml*:\n",
    "\n",
    "```yaml\n",
    "add_features:\n",
    "  time:\n",
    "    type: datetime\n",
    "    subset: month, hourofweek, hour\n",
    "  \n",
    "regressors:\n",
    "  month:\n",
    "    feature: month\n",
    "    type: categorical\n",
    "    encode_as: onehot\n",
    "\n",
    "  tow:\n",
    "    feature: hourofweek\n",
    "    type: categorical\n",
    "    max_n_categories: 60  \n",
    "    encode_as: onehot\n",
    "\n",
    "  hour:\n",
    "    feature: hour\n",
    "    type: categorical\n",
    "    max_n_categories: 12  \n",
    "    encode_as: onehot\n",
    "    interaction_only: true\n",
    "\n",
    "  holidays:\n",
    "    feature: holiday\n",
    "    type: categorical\n",
    "    max_n_categories: 3\n",
    "    excluded_categories: _novalue_ # default value for imputing missing categorical data\n",
    "    stratify_by: hour \n",
    "    min_samples_leaf: 1\n",
    "    interaction_only: true\n",
    "  \n",
    "  lin_temperature:\n",
    "    feature: temperature\n",
    "    type: linear\n",
    "  \n",
    "  flex_temperature:\n",
    "    feature: temperature\n",
    "    type: spline\n",
    "    n_knots: 5\n",
    "    degree: 1\n",
    "    strategy: uniform \n",
    "    extrapolation: constant\n",
    "    interaction_only: true\n",
    "\n",
    "interactions:\n",
    "  hour, holidays: ~\n",
    "  tow, flex_temperature:\n",
    "    tow:\n",
    "      max_n_categories: 2 \n",
    "      stratify_by: temperature \n",
    "      min_samples_leaf: 15 \n",
    "```\n",
    "\n",
    "This models add to the `towt` model an intercation term between the holidays feature and the hour of the day, so that to correct the contribution of the hour of the week based on whether the day is a holiday or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "83c5a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = load_catalog('demo', model='towt_holidays')\n",
    "model_structure = catalog.load('model_structure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "4bac9c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train.loc[~data_train['consumption_outlier'], ['temperature', 'holiday']]\n",
    "y_train = data_train.loc[X_train.index, ['consumption']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "976c36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = ClusterFeatures(min_cluster_size=20, \n",
    "                            transformer=mcc_features, \n",
    "                            output_name='cluster')\n",
    "\n",
    "reg_grouped = GroupedPredictor(model_structure=model_structure, \n",
    "                               group_feature='cluster',\n",
    "                               estimator_params=(('alpha', 0.01), ('fit_intercept', False)),\n",
    ")\n",
    "\n",
    "model = CompositePredictor(distance_metrics=distance_metrics, \n",
    "                           base_clusterer=clusterer,\n",
    "                           base_regressor=reg_grouped,\n",
    "                           group_feature='cluster'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccfda2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "budget = 15\n",
    "\n",
    "parallel = Parallel(n_jobs=2)\n",
    "results = parallel(\n",
    "        delayed(optimize_for_tag)(\n",
    "                clone(model), \n",
    "                X_train, \n",
    "                y_train,\n",
    "                budget, \n",
    "                tag\n",
    "        ) for tag in ['eom', 'leaf']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a54c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = None\n",
    "params= None\n",
    "\n",
    "for res in results:\n",
    "    scores = pd.concat((scores, res['scores']), ignore_index=True)\n",
    "    params = pd.concat(\n",
    "    ( params,\n",
    "      res['params'].assign(**{'assign_clusters__cluster_selection_method': res['tags']})\n",
    "    ), ignore_index=True)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "da47e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_include = scores.sort_values(by='CVRMSE').iloc[:5]\n",
    "params_ = params.loc[to_include.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "3ba6a802",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_model = EnsemblePredictor(base_estimator=model,\n",
    "                             ensemble_parameters=params_.to_dict('records')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e2ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "en_model = en_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pred = en_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea4a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_train\n",
    "\n",
    "print(f\"In-sample CV(RMSE) (%): {cvrmse(y_true[['consumption']], pred[['consumption']])*100}\")\n",
    "print(f\"In-sample NMBE (%): {nmbe(y_true[['consumption']], pred[['consumption']])*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a64616",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_model.n_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43688dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = y_true[['consumption']] - pred[['consumption']]\n",
    "resid = StandardScaler().fit_transform(resid).squeeze()\n",
    "resid = pd.Series(data=resid, index=y_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2538fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "discord_resids = resid.loc[discords.index]\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    grid = sns.jointplot(x=pred['consumption'], y=resid)\n",
    "    sns.scatterplot(x=pred.loc[discord_resids.index, 'consumption'], y=discord_resids, \n",
    "                    color='#feb24c', ax=grid.ax_joint)\n",
    "    grid.fig.set_figwidth(12)\n",
    "    grid.fig.set_figheight(5)\n",
    "    grid.set_axis_labels(xlabel='Predicted Value', ylabel='Standardized Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1630990",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d992a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
