{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48bf8276",
   "metadata": {},
   "source": [
    "# The `eensight` functionality for encoding categorical and numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b87506",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f82bf202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0d221ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eensight.utils import add_constant, tensor_product\n",
    "\n",
    "from eensight.features.encode import IdentityEncoder, TrendFeatures\n",
    "from eensight.features.encode import DatetimeFeatures, CyclicalFeatures, MMCFeatures\n",
    "from eensight.features.encode import SafeOrdinalEncoder, SafeOneHotEncoder\n",
    "from eensight.features.encode import TargetClusterEncoder, CategoricalEncoder\n",
    "from eensight.features.encode import ICatEncoder, SplineEncoder, ISplineEncoder\n",
    "from eensight.features.encode import ProductEncoder, ICatLinearEncoder, ICatSplineEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40e4c97",
   "metadata": {},
   "source": [
    "## Utility encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940b3bbb",
   "metadata": {},
   "source": [
    "### `eensight.features.encode.IdentityEncoder` \n",
    "\n",
    "The identity encoder returns what it is fed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature : str or list of str, default=None\n",
    "        The name(s) of the input dataframe's column(s) to return.\n",
    "        If None, the whole input dataframe is returned.\n",
    "    include_bias : bool, default=False\n",
    "        If True, a column of ones is added to the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daf9efc",
   "metadata": {},
   "source": [
    "We can create a dummy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b0a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(index=pd.date_range(start='1/1/2018', end='31/12/2019', freq='H'))\n",
    "data['day'] = data.index.dayofyear\n",
    "data['x1'] = 4 + 3*np.sin(data['day']/365*2*np.pi)\n",
    "data['x2'] = 4 * np.sin(data['day']/365*4*np.pi+365/2)\n",
    "noise = np.random.normal(0, 0.9, len(data))\n",
    "data['y'] = data['x1'] + data['x2'] + noise\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52af30",
   "metadata": {},
   "source": [
    "All encoders have a `n_features_out_` after fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c2acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = IdentityEncoder(feature='x1', include_bias=True)\n",
    "assert enc.fit_transform(data).shape[1] == enc.n_features_out_\n",
    "enc.n_features_out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00159af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = IdentityEncoder(feature='x1')\n",
    "assert np.allclose(enc.fit_transform(data), data[['x1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5c4119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = IdentityEncoder(include_bias=True)\n",
    "assert np.allclose(enc.fit_transform(data), add_constant(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf2c80",
   "metadata": {},
   "source": [
    "## Creating time trend features \n",
    "\n",
    "### `eensight.features.encode.TrendFeatures` \n",
    "\n",
    "Generates time trend features.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature : str, default=None\n",
    "        The name of the input dataframe's column that contains datetime information.\n",
    "        If None, it is assumed that the datetime information is provided by the\n",
    "        input dataframe's index.\n",
    "    include_bias : bool, default=False\n",
    "        If True, a column of ones is added to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b146cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = TrendFeatures(include_bias=True)\n",
    "features = enc.fit_transform(data)\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):    \n",
    "    fig = plt.figure(figsize=(14, 3.54), dpi=96)\n",
    "    layout = (2, 1)\n",
    "    ax1 = plt.subplot2grid(layout, (0, 0))\n",
    "    ax2 = plt.subplot2grid(layout, (1, 0))\n",
    "    \n",
    "    ax1.plot(features[:, 0])\n",
    "    ax2.plot(features[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5528b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert enc.n_features_out_ == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f4d636",
   "metadata": {},
   "source": [
    "## Adding date and time features\n",
    "\n",
    "### `eensight.features.encode.DatetimeFeatures` \n",
    "\n",
    "Generates date and time features\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature : str, default=None\n",
    "        The name of the input dataframe's column that contains datetime information.\n",
    "        If None, it is assumed that the datetime information is provided by the\n",
    "        input dataframe's index.\n",
    "    remainder : str, :type : {'drop', 'passthrough'}, default='drop'\n",
    "        By specifying ``remainder='passthrough'``, all the remaining columns of the\n",
    "        input dataset will be automatically passed through (concatenated with the\n",
    "        output of the transformer).\n",
    "    replace : bool, default=False\n",
    "        Specifies whether replacing an existing column with the same name is allowed\n",
    "        (when `remainder=passthrough`).\n",
    "    subset : str or list of str (default=None)\n",
    "        The names of the features to generate. If None, all features will be produced:\n",
    "        'month', 'week', 'dayofyear', 'dayofweek', 'hour', 'hourofweek', 'time'.\n",
    "        The last 3 features are generated only if the timestep of the input's\n",
    "        `feature` (or index if `feature` is None) is smaller than `pd.Timedelta(days=1)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d22ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = DatetimeFeatures(remainder='drop')\n",
    "features = enc.fit_transform(data)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d4fd0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert features.shape[1] == enc.n_features_out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecf3259",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = DatetimeFeatures(remainder='passthrough')\n",
    "features = enc.fit_transform(data)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de63242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert features.shape[1] == enc.n_features_out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e7abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = DatetimeFeatures(remainder='drop', subset=['month', 'dayofweek', 'hourofweek'])\n",
    "enc.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657b607b",
   "metadata": {},
   "source": [
    "## Encoding cyclical (seasonal) features\n",
    "\n",
    "### `eensight.features.encode.CyclicalFeatures` \n",
    "\n",
    "Creates cyclical (seasonal) features as fourier terms\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature : str, default=None\n",
    "        The name of the input dataframe's column that contains datetime information.\n",
    "        If None, it is assumed that the datetime information is provided by the\n",
    "        input dataframe's index.\n",
    "    seasonality : str, default=None\n",
    "        The name of the seasonality.\n",
    "    period : float\n",
    "        Number of days in one period.\n",
    "    fourier_order : int\n",
    "        Number of Fourier components to use.\n",
    "\n",
    "**Note**: The encoder can provide default values for `period` and `fourier_order` if `seasonality` is one of `daily`, `weekly` or `yearly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041825ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):    \n",
    "    fig = plt.figure(figsize=(14, 3.54), dpi=96)\n",
    "    layout = (1, 1)\n",
    "    ax = plt.subplot2grid(layout, (0, 0))\n",
    "    \n",
    "    data['y'].plot(ax=ax, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ce9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = CyclicalFeatures(seasonality='yearly', fourier_order=3)\n",
    "enc.fit(data)\n",
    "enc.n_features_out_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba28b659",
   "metadata": {},
   "source": [
    "Now letâ€™s plot our transformed features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927841ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_trf = pd.DataFrame(data=enc.transform(data), index=data.index)\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):    \n",
    "    fig, axs = plt.subplots(enc.n_features_out_, figsize=(14, 7), dpi=96)\n",
    "    \n",
    "    for i, col in enumerate(feature_trf.columns):\n",
    "        feature_trf[col].plot(ax=axs[i])\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555359c4",
   "metadata": {},
   "source": [
    "Let's see how well this transformation works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "414c579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression().fit(feature_trf, data['y'])\n",
    "pred = regr.predict(feature_trf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25e5ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):    \n",
    "    fig = plt.figure(figsize=(14, 3.54), dpi=96)\n",
    "    layout = (1, 1)\n",
    "    ax = plt.subplot2grid(layout, (0, 0))\n",
    "    \n",
    "    data['y'].plot(ax=ax, alpha=0.5)\n",
    "    pd.Series(pred, index=data.index).plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c299f6",
   "metadata": {},
   "source": [
    "The root mean squared error is very close to the noise that was injected in the data (0.9):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749ec4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(data['y'], pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fa2cf0",
   "metadata": {},
   "source": [
    "## Creating calendar features for metric learning\n",
    "\n",
    "### `eensight.features.encode.MMCFeatures` \n",
    "\n",
    "Generates `month` and `dayofweek` one-hot encoded features for training an MMC\n",
    "    (Mahalanobis Metric for Clustering) metric learning algorithm. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    month_feature : str, default='month'\n",
    "        The name of the input dataframe's column that contains the `month` feature values.\n",
    "    dow_feature : str, default='dayofweek'\n",
    "        The name of the input dataframe's column that contains the `dayofweek` feature \n",
    "        values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5034009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Pipeline([\n",
    "    ('add_date_features', DatetimeFeatures(subset=['month', 'dayofweek'])),\n",
    "    ('add_mmc_features',  MMCFeatures())\n",
    "])\n",
    "\n",
    "\n",
    "enc.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f749f6b",
   "metadata": {},
   "source": [
    "## Encoding categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2acd30",
   "metadata": {},
   "source": [
    "### A dataset to demonstrate the functionality\n",
    "\n",
    "The dataset is related to the two-year historical log corresponding to years 2011 and 2012 from Capital Bikeshare system, Washington D.C., USA which is publicly available in http://capitalbikeshare.com/system-data. The dataset includes:\n",
    "\n",
    "- `instant` : record index\n",
    "\n",
    "\n",
    "- `dteday` : date\n",
    "\n",
    "\n",
    "- `season` : season (1:springer, 2:summer, 3:fall, 4:winter)\n",
    "\n",
    "\n",
    "- `yr` : year (0: 2011, 1:2012)\n",
    "\n",
    "\n",
    "- `mnth` : month ( 1 to 12)\n",
    "\n",
    "\n",
    "- `hr` : hour (0 to 23)\n",
    "\n",
    "\n",
    "- `holiday` : weather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)\n",
    "\n",
    "\n",
    "- `weekday` : day of the week\n",
    "\n",
    "\n",
    "- `workingday` : if day is neither weekend nor holiday is 1, otherwise is 0.\n",
    "\n",
    "\n",
    "- `weathersit` : \n",
    "    - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "\t- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "\t- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "\t- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "\n",
    "\n",
    "- `temp` : Normalized temperature in Celsius. The values are divided by 41 (max)\n",
    "\n",
    "\n",
    "- `atemp`: Normalized feeling temperature in Celsius. The values are divided by 50 (max)\n",
    "\n",
    "\n",
    "- `hum` : Normalized humidity. The values are divided by 100 (max)\n",
    "\n",
    "\n",
    "- `windspeed` : Normalized wind speed. The values are divided to by (max)\n",
    "\n",
    "\n",
    "- `casual` : count of casual users\n",
    "\n",
    "\n",
    "- `registered` : count of registered users\n",
    "\n",
    "\n",
    "- `cnt` : count of total rental bikes including both casual and registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'play_data/bike_sharing/hour.csv'\n",
    "data = pd.read_csv(path)\n",
    "data = data.drop(['instant', 'casual', 'registered'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "126c25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('cnt', axis=1)\n",
    "y = data['cnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a570d33e",
   "metadata": {},
   "source": [
    "### `eensight.features.encode.SafeOrdinalEncoder` \n",
    "\n",
    "Encodes categorical features as an integer array. The encoder converts the \n",
    "features into ordinal integers. This results in a single column of integers \n",
    "(0 to n_categories - 1) per feature.\n",
    "\n",
    "It is implemented as a pipeline that connects a `sklearn.preprocessing.OrdinalEncoder` with a `sklearn.impute.SimpleImputer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature : str or list of str, default=None\n",
    "        The names of the columns to encode. If None, all categorical columns will\n",
    "        be encoded.\n",
    "    unknown_value : int, default=None\n",
    "        This parameter will set the encoded value for unknown categories. It has to\n",
    "        be distinct from the values used to encode any of the categories in `fit`.\n",
    "        If None, the value `-1` is used. During `transform`, unknown categories\n",
    "        will be replaced using the most frequent value along each column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef032c6",
   "metadata": {},
   "source": [
    "Let's fit on all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "819cea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = SafeOrdinalEncoder()\n",
    "enc = enc.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6989bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in enc.features_:\n",
    "    print('{0:10} is {1}'.format(feature, X.dtypes[feature]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47c0991",
   "metadata": {},
   "source": [
    "By default, the encoder regards as categorical any feature that is of type `object`, `bool`, `integer` or `category`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ada115",
   "metadata": {},
   "source": [
    "Next, let's encode only the `weathersit` column, by first fitting on data that does not include `weathersit=4` and then transforming all data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f82d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The most frequent value is {X['weathersit'].mode().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82ce44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = SafeOrdinalEncoder(feature='weathersit')\n",
    "enc = enc.fit(X[X['weathersit'] != 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da76e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_trf = enc.transform(X)\n",
    "feature_trf = pd.Series(data=feature_trf.squeeze(), index=X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f769b44e",
   "metadata": {},
   "source": [
    "`weathersit=1` is mapped to 0, `weathersit=2` is mapped to 1, `weathersit=3` is mapped to 2, and `weathersit=4` is mapped where the most frequent value is mapped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b530ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"weathersit=1 is mapped to: {feature_trf[X[X['weathersit']==1].index].unique().item()}\")\n",
    "print(f\"weathersit=2 is mapped to: {feature_trf[X[X['weathersit']==2].index].unique().item()}\")\n",
    "print(f\"weathersit=3 is mapped to: {feature_trf[X[X['weathersit']==3].index].unique().item()}\")\n",
    "print(f\"weathersit=4 is mapped to: {feature_trf[X[X['weathersit']==4].index].unique().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced49202",
   "metadata": {},
   "source": [
    "### `eensight.features.encode.SafeOneHotEncoder` \n",
    "\n",
    "The encoder uses a `SafeOrdinalEncoder`to first encode the feature as an integer \n",
    "array and then a `sklearn.preprocessing.OneHotEncoder` to encode the features as \n",
    "an one-hot array. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature : str or list of str, default=None\n",
    "        The names of the columns to encode. If None, all categorical columns will\n",
    "        be encoded.\n",
    "    unknown_value : int, default=None\n",
    "        This parameter will set the encoded value of unknown categories. It has to\n",
    "        be distinct from the values used to encode any of the categories in `fit`.\n",
    "        If None, the value `-1` is used. During `transform`, unknown categories\n",
    "        will be replaced using the most frequent value along each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69495cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = SafeOneHotEncoder(feature='weathersit')\n",
    "enc = enc.fit(X[X['weathersit'] != 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d64d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The encoder will transform the input column into {enc.n_features_out_} new ones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1d198c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_trf = enc.transform(X)\n",
    "feature_trf = pd.DataFrame(data=feature_trf, index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d31838",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The 1st column of the transformed feature corresponds to the \"\n",
    "      f\"{X.loc[feature_trf.iloc[:, 0] == 1, 'weathersit'].unique()} categories\")\n",
    "\n",
    "print(\"The 2nd column of the transformed feature corresponds to the \"\n",
    "      f\"{X.loc[feature_trf.iloc[:, 1] == 1, 'weathersit'].unique()} categories\")\n",
    "\n",
    "print(\"The 3rd column of the transformed feature corresponds to the \"\n",
    "      f\"{X.loc[feature_trf.iloc[:, 2] == 1, 'weathersit'].unique()} categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac6b3d",
   "metadata": {},
   "source": [
    "### `eensight.features.encode.TargetClusterEncoder` \n",
    "\n",
    "Encodes a categorical feature as clusters of the target's values so that \n",
    "    to reduce its cardinality.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature : str\n",
    "        The name of the categorical feature to transform. This encoder operates \n",
    "        on a single feature.\n",
    "    max_n_categories : int\n",
    "        The maximum number of categories to produce.\n",
    "    stratify_by : str or list of str (default=None)\n",
    "        If not None, the encoder will first stratify the categorical feature into\n",
    "        groups that have similar values of the features in `stratify_by`, and then\n",
    "        cluster based on the relationship between the categorical feature and the\n",
    "        target.\n",
    "    excluded_categories : str or list of str (default=None)\n",
    "        The names of the categories to be excluded from the clustering process. These\n",
    "        categories will stay intact by the encoding process, so they cannot have the\n",
    "        same values as the encoder's results (the encoder acts as an ``OrdinalEncoder``\n",
    "        in the sense that the feature is converted into a column of integers 0 to\n",
    "        n_categories - 1).\n",
    "    unknown_value : int, default=None\n",
    "        This parameter will set the encoded value of unknown categories. It has to\n",
    "        be distinct from the values used to encode any of the categories in `fit`.\n",
    "        If None, the value `-1` is used.\n",
    "    min_samples_leaf : int, default=1\n",
    "        The minimum number of samples required to be at a leaf node of the decision\n",
    "        tree model that is used for stratifying the categorical feature if `stratify_by`\n",
    "        is not None. The actual number that will be passed to the tree model is\n",
    "        `min_samples_leaf` multiplied by the number of unique values in the categorical\n",
    "        feature to transform.\n",
    "    max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
    "        The number of features to consider when looking for the best split:\n",
    "        - If int, then consider `max_features` features at each split of the decision tree.\n",
    "        - If float, then `max_features` is a fraction and\n",
    "        `int(max_features * n_features)` features are considered at each\n",
    "        split.\n",
    "        - If \"auto\", then `max_features=n_features`.\n",
    "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
    "        - If \"log2\", then `max_features=log2(n_features)`.\n",
    "        - If None, then `max_features=n_features`.\n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        Controls the randomness of the estimator. To obtain a deterministic behaviour\n",
    "        during fitting, ``random_state`` has to be fixed to an integer.\n",
    "\n",
    "**Note**:\n",
    "    This encoder does not replace unknown values with the most frequent one during\n",
    "    `transform`. It just assigns them the value of `unknown_value`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a4086",
   "metadata": {},
   "source": [
    "We can treat the `hr` feature as categorical and see if we can lump together some of its distinct values so that we reduce their number from 24 (hours of day) to 4. \n",
    "\n",
    "First, we can look at how the target changes for each `hr` value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67768e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_group = pd.concat((y, X['hr']), axis=1)\n",
    "grouped_mean = to_group.groupby('hr').mean()\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):    \n",
    "    fig = plt.figure(figsize=(14, 3.54), dpi=96)\n",
    "    layout = (1, 1)\n",
    "    ax = plt.subplot2grid(layout, (0, 0))\n",
    "\n",
    "    grouped_mean.plot.bar(ax=ax, rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f90e60",
   "metadata": {},
   "source": [
    "One approach could be to group `hr` values together according to the different levels of the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d339a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = KBinsDiscretizer(n_bins=4, encode='ordinal')\n",
    "bins = disc.fit_transform(grouped_mean)\n",
    "grouped_mean['bins'] = bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d925043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_values = [0, 1, 2, 3]\n",
    "color_list = ['#74a9cf', '#fc8d59', '#9e9ac8', '#a1d99b']\n",
    "b2c = dict(zip(bin_values, color_list))\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):    \n",
    "    fig = plt.figure(figsize=(14, 3.54), dpi=96)\n",
    "    layout = (1, 1)\n",
    "    ax = plt.subplot2grid(layout, (0, 0))\n",
    "\n",
    "    grouped_mean['cnt'].plot.bar(ax=ax, rot=0, color=[b2c[i] for i in grouped_mean['bins']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d325349",
   "metadata": {},
   "source": [
    "Going one step further, we could examine not only the mean of the target per `hr` value but also other characteristics of the distribution. As an example, while `hr=7` and `hr=12` are lumbed into the same category, they have quite different distributions, which may or may not be an issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f9f4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 3.54))\n",
    "\n",
    "    to_group[to_group['hr']==7]['cnt'].plot.hist(ax=ax1, bins=30, alpha=0.5)\n",
    "    to_group[to_group['hr']==12]['cnt'].plot.hist(ax=ax2, bins=30, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab92d485",
   "metadata": {},
   "source": [
    "To take more aspects of the target's distribution into account, the `eensight.features.encode.TargetClusterEncoder` clusters the different values of a categorical feature according to the mean, standard deviation, skewness and [Wasserstein distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wasserstein_distance.html) of the distribution of the corresponding target's values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49a29041",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = TargetClusterEncoder(feature='hr', max_n_categories=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc38646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = enc.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04d4eae",
   "metadata": {},
   "source": [
    "We can update the `bins` column based on the encoder's mapping between values of `hr` and clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "247d6bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_mean['bins'] = grouped_mean.index.map(lambda x: enc.mapping_[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc02b6c",
   "metadata": {},
   "source": [
    "... and plot the new features again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa4f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_values = [0, 1, 2, 3]\n",
    "color_list = ['#74a9cf', '#fc8d59', '#9e9ac8', '#a1d99b']\n",
    "b2c = dict(zip(bin_values, color_list))\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):    \n",
    "    fig = plt.figure(figsize=(14, 3.54), dpi=96)\n",
    "    layout = (1, 1)\n",
    "    ax = plt.subplot2grid(layout, (0, 0))\n",
    "\n",
    "    grouped_mean['cnt'].plot.bar(ax=ax, rot=0, color=[b2c[i] for i in grouped_mean['bins']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ecc7d",
   "metadata": {},
   "source": [
    "In order to judge the value of the encoding scheme, we will compare three cases:\n",
    "1. A random forest regressor is fitted on data without encoding\n",
    "2. A random forest regressor is fitted on data with encoding based only on mean target values\n",
    "3. A random forest regressor is fitted on data with encoding based on target value distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a4d2ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.drop('dteday', axis=1), y, \n",
    "                                                    test_size=0.33, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ed40cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88254461",
   "metadata": {},
   "source": [
    "#### 1. A random forest regressor is fitted on data without encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4e67dcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor(n_estimators=500, criterion='mse', \n",
    "                             min_samples_leaf=1, max_features='auto', \n",
    "                             bootstrap=True, max_samples=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fccca0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, regr.predict(X_test))\n",
    "scores.append(mse)\n",
    "print(f'Mean squared out-of-sample error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efb941a",
   "metadata": {},
   "source": [
    "#### 2. A random forest regressor is fitted on data with encoding based only on mean target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "60f5e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_group = pd.concat((y_train, X_train['hr']), axis=1)\n",
    "grouped_mean = to_group.groupby('hr').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2232487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = KBinsDiscretizer(n_bins=4, encode='ordinal')\n",
    "bins = enc.fit_transform(grouped_mean)\n",
    "grouped_mean['bins'] = bins.astype(int)\n",
    "mapping = grouped_mean['bins'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d16ee929",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = X_train.copy()\n",
    "X_test_ = X_test.copy()\n",
    "\n",
    "X_train_['hr'] = X_train_['hr'].map(lambda x: mapping[x])\n",
    "X_test_['hr'] = X_test_['hr'].map(lambda x: mapping[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c6e12337",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = regr.fit(X_train_, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fdfd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, regr.predict(X_test_))\n",
    "scores.append(mse)\n",
    "print(f'Mean squared out of sample error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fda5e6",
   "metadata": {},
   "source": [
    "#### 3. A random forest regressor is fitted on data with encoding based on target value distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e82fdd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = X_train.copy()\n",
    "X_test_ = X_test.copy()\n",
    "\n",
    "enc = TargetClusterEncoder(feature='hr', max_n_categories=4)\n",
    "X_train_['hr'] = enc.fit_transform(X_train_, y_train)\n",
    "X_test_['hr'] = enc.transform(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a388125",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = regr.fit(X_train_, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e29d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, regr.predict(X_test_))\n",
    "scores.append(mse)\n",
    "print(f'Mean squared out of sample error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b088fd",
   "metadata": {},
   "source": [
    "We lose predictive power by reducing the cardinality of the `hr` feature, but we get better results when taking into consideration the overall distribution of the target's data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42faaca9",
   "metadata": {},
   "source": [
    "#### Conditional effect on target\n",
    "\n",
    "Furthermore, one may be interested in clustering the `hr` feature taking into account the `weekday` feature: how similar are the target's values in two distinct values of `hr` given similar values for `weekday`?\n",
    "\n",
    "In this case, the encoder first stratifies the categorical feature `hr` into groups with similar values of `weekday`, and then examines the relationship between the categorical feature's values and the corresponding values of the target.\n",
    "\n",
    "The stratification is carried out by a `sklearn.tree.DecisionTreeRegressor` model that first fits the `stratify_by` features (here `weekday`) on the target values, and then uses the tree's leaf nodes as groups. \n",
    "\n",
    "Only the mean of the target's values per group is taken into account when deriving the clusters. \n",
    "\n",
    "The parameter `min_samples_leaf` defines the minimum number of samples required to be at a leaf node of the decision tree model. Note that the actual number that will be passed to the tree model is `min_samples_leaf` multiplied by the number of unique values in the categorical feature to transform.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f2f42b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = TargetClusterEncoder(feature='hr', max_n_categories=4, stratify_by='weekday',\n",
    "                           min_samples_leaf=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "26be692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = X_train.copy()\n",
    "X_test_ = X_test.copy()\n",
    "\n",
    "X_train_['hr'] = enc.fit_transform(X_train_, y_train)\n",
    "X_test_['hr'] = enc.transform(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7b4c58f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = regr.fit(X_train_, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc395b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, regr.predict(X_test_))\n",
    "scores.append(mse)\n",
    "print(f'Mean squared out of sample error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257365de",
   "metadata": {},
   "source": [
    "We get even better results while still encoding the `hr` feature with only 4 levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c0b8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(list(enc.mapping_.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456a6625",
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):    \n",
    "    fig = plt.figure(figsize=(14, 3.54), dpi=96)\n",
    "    layout = (1, 1)\n",
    "    ax = plt.subplot2grid(layout, (0, 0))\n",
    "\n",
    "    pd.Series(scores, index=['All 24 values', '4 values - only target mean',  \n",
    "                             '4 values - target distribution', '4 values - conditional effect']\n",
    "    ).plot.bar(ax=ax, rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0add4",
   "metadata": {},
   "source": [
    "An alternative way to understand the `stratify_by` effect is to cluster the \"day of week\" feature into 3 categories by taking into account the impact of the \"hour of the day\" feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ab698852",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = TargetClusterEncoder(feature='weekday', max_n_categories=3, stratify_by='hr',\n",
    "                           min_samples_leaf=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e9334b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = enc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "67f5767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_group = pd.concat((y, X['weekday']), axis=1)\n",
    "grouped_mean = to_group.groupby('weekday').mean()\n",
    "grouped_mean['bins'] = grouped_mean.index.map(lambda x: enc.mapping_[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da25c194",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_values = [0, 1, 2]\n",
    "color_list = ['#74a9cf', '#fc8d59', '#9e9ac8']\n",
    "b2c = dict(zip(bin_values, color_list))\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):    \n",
    "    fig = plt.figure(figsize=(14, 2.54), dpi=96)\n",
    "    layout = (1, 1)\n",
    "    ax = plt.subplot2grid(layout, (0, 0))\n",
    "\n",
    "    grouped_mean['cnt'].plot.bar(ax=ax, rot=0, color=[b2c[i] for i in grouped_mean['bins']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa80303c",
   "metadata": {},
   "source": [
    "The mean values provide absolutely no information on why the days were grouped in the way they did. However, it is easy to understand the result if we consider that when the encoder groups days stratified by hours, it actually groups the daily profiles of the target's value per hour: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a261c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = pd.concat((y, X[['weekday', 'hr']]), axis=1).groupby(['weekday', 'hr'])  \\\n",
    "             .mean().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc9c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):    \n",
    "    fig = plt.figure(figsize=(14, 3.5), dpi=96)\n",
    "    layout = (1, 1)\n",
    "    ax = plt.subplot2grid(layout, (0, 0))\n",
    "    \n",
    "    for day, values in profiles.iterrows():\n",
    "        values.plot(ax=ax, color=b2c[enc.mapping_[day]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d00a57",
   "metadata": {},
   "source": [
    "### `eensight.features.encode.CategoricalEncoder` \n",
    "\n",
    "Encodes a categorical feature by encaptulating all the aforementioned categorical encoders so far.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature : str\n",
    "        The name of the categorical feature to transform. This encoder operates on \n",
    "        a single feature.\n",
    "    max_n_categories : int (default=None)\n",
    "        The maximum number of categories to produce.\n",
    "    stratify_by : str or list of str (default=None)\n",
    "        If not None, the encoder will first stratify the categorical feature into\n",
    "        groups that have similar values of the features in `stratify_by`, and then\n",
    "        cluster based on the relationship between the categorical feature and the\n",
    "        target.\n",
    "    excluded_categories : str or list of str (default=None)\n",
    "        The names of the categories to be excluded from the clustering process. These\n",
    "        categories will stay intact by the encoding process, so they cannot have the\n",
    "        same values as the encoder's results (the encoder acts as an ``OrdinalEncoder``\n",
    "        in the sense that the feature is converted into a column of integers 0 to\n",
    "        n_categories - 1).\n",
    "    unknown_value : int, default=None\n",
    "        This parameter will set the encoded value of unknown categories. It has to\n",
    "        be distinct from the values used to encode any of the categories in `fit`.\n",
    "        If None, the value `-1` is used.\n",
    "    min_samples_leaf : int, default=1\n",
    "        The minimum number of samples required to be at a leaf node of the decision\n",
    "        tree model that is used for stratifying the categorical feature if `stratify_by`\n",
    "        is not None. The actual number that will be passed to the tree model is\n",
    "        `min_samples_leaf` multiplied by the number of unique values in the categorical\n",
    "        feature to transform.\n",
    "    max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
    "        The number of features to consider when looking for the best split:\n",
    "        - If int, then consider `max_features` features at each split of the decision tree.\n",
    "        - If float, then `max_features` is a fraction and\n",
    "        `int(max_features * n_features)` features are considered at each\n",
    "        split.\n",
    "        - If \"auto\", then `max_features=n_features`.\n",
    "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
    "        - If \"log2\", then `max_features=log2(n_features)`.\n",
    "        - If None, then `max_features=n_features`.\n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        Controls the randomness of the estimator. To obtain a deterministic behaviour\n",
    "        during fitting, ``random_state`` has to be fixed to an integer.\n",
    "    encode_as : str {'onehot', 'ordinal'}, default='onehot'\n",
    "        Method used to encode the transformed result.\n",
    "        onehot\n",
    "            Encode the transformed result with one-hot encoding\n",
    "            and return a dense array.\n",
    "        ordinal\n",
    "            Encode the transformed result as integer values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83ce6f6",
   "metadata": {},
   "source": [
    "If `max_n_categories` is not None and the number of distinct values of the categorical feature is larger than the `max_n_categories`, the `eensight.features.encode.TargetClusterEncoder` will be called. \n",
    "\n",
    "If `encode_as = 'onehot'`, the result comes from a `eensight.features.encode.TargetClusterEncoder` + `eensight.features.encode.SafeOneHotEncoder` pipeline, otherwise from `eensight.features.encode.TargetClusterEncoder` + `eensight.features.encode.SafeOrdinalEncoder`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "490e9f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_categories = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac4aecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = CategoricalEncoder(feature='weekday', max_n_categories=max_n_categories, \n",
    "                         encode_as='onehot')\n",
    "feature_trf = enc.fit_transform(X, y)\n",
    "feature_trf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d00199c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert min(X['weekday'].nunique(), max_n_categories) == enc.n_features_out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cdf61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = CategoricalEncoder(feature='weekday', max_n_categories=10, encode_as='ordinal')\n",
    "feature_trf = enc.fit_transform(X, y)\n",
    "feature_trf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dde3b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert min(X['weekday'].nunique(), max_n_categories) == np.unique(feature_trf).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6713e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_categories = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253e53a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = CategoricalEncoder(feature='weekday', max_n_categories=max_n_categories, \n",
    "                         encode_as='onehot')\n",
    "feature_trf = enc.fit_transform(X, y)\n",
    "feature_trf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3ce0781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert min(X['weekday'].nunique(), max_n_categories) == enc.n_features_out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743a5b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = CategoricalEncoder(feature='weekday', max_n_categories=max_n_categories, \n",
    "                         encode_as='ordinal')\n",
    "\n",
    "feature_trf = enc.fit_transform(X, y)\n",
    "feature_trf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a15bc084",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert min(X['weekday'].nunique(), max_n_categories) == np.unique(feature_trf).size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf560f",
   "metadata": {},
   "source": [
    "## Encoding pairwise interactions between categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a4df78",
   "metadata": {},
   "source": [
    "### `eensight.features.encode.ICatEncoder` \n",
    "\n",
    "Encodes the interaction between two categorical features\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoder_left : eensight.features.encode.CategoricalEncoder\n",
    "        The encoder for the first of the two features\n",
    "    encoder_right : eensight.features.encode.CategoricalEncoder\n",
    "        The encoder for the second of the two features\n",
    "\n",
    "**Note**: Both encoders should have the same `encode_as` parameter.\n",
    "If one or both of the encoders is already fitted, it will not be re-fitted during `fit` or `fit_transform`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf232eb",
   "metadata": {},
   "source": [
    "If `encode_as = 'onehot'`, it returns the tensor product of the results of the two encoders. The tensor product combines row-per-row the results from the first and the second encoder as follows:\n",
    "\n",
    "![tensor product](images/tensor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530fd23a",
   "metadata": {},
   "source": [
    "A small example of the tensor product function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd0788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 10]).reshape(1, -1)\n",
    "b = np.array([10, 20, 30]).reshape(1, -1)\n",
    "\n",
    "tensor_product(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90adeb9e",
   "metadata": {},
   "source": [
    "The easiest way to demonstate it is by combining hours of day and days of week into hours of week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfcd455",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_day = CategoricalEncoder(feature='weekday', encode_as='onehot')\n",
    "feature_trf = enc_day.fit_transform(X)\n",
    "feature_trf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e0a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_hour = CategoricalEncoder(feature='hr', encode_as='onehot')\n",
    "feature_trf = enc_hour.fit_transform(X)\n",
    "feature_trf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "545303de",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = ICatEncoder(enc_day, enc_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc7c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We expect {7*24} columns in the interactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58531f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_trf = enc.fit_transform(X)\n",
    "feature_trf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d362ad",
   "metadata": {},
   "source": [
    "Make sure it is a proper one-hot array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f1fac3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(feature_trf.sum(axis=1), np.ones(len(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d8532",
   "metadata": {},
   "source": [
    "If `encode_as = 'ordinal'`, it returns the combinations of the encoders' results, where each combination is a string with `:` between the two values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa235d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_day = CategoricalEncoder(feature='weekday', encode_as='ordinal')\n",
    "enc_hour = CategoricalEncoder(feature='hr', encode_as='ordinal')\n",
    "enc = ICatEncoder(enc_day, enc_hour)\n",
    "\n",
    "feature_trf = enc.fit_transform(X)\n",
    "feature_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "aeae099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.unique(feature_trf).size == 168"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff7b35",
   "metadata": {},
   "source": [
    "## Encoding numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9174b2",
   "metadata": {},
   "source": [
    "### `eensight.features.encode.SplineEncoder` \n",
    "\n",
    "Encodes a numerical feature by generating univariate B-spline bases.\n",
    "It generates a new feature matrix consisting of `n_splines=n_knots + degree - 1` \n",
    "spline basis functions (B-splines) of polynomial order=`degree` for each feature.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature : str\n",
    "        The name of the column to encode.\n",
    "    n_knots : int, default=5\n",
    "        Number of knots of the splines if `knots` equals one of {'uniform', 'quantile'}.\n",
    "        Must be larger or equal 2. Ignored if `knots` is array-like.\n",
    "    degree : int, default=3\n",
    "        The polynomial degree of the spline basis. Must be a non-negative integer.\n",
    "    strategy : {'uniform', 'quantile'} or array-like of shape (n_knots, n_features),\n",
    "        default='quantile'\n",
    "        Set knot positions such that first knot <= features <= last knot.\n",
    "        - If 'uniform', `n_knots` number of knots are distributed uniformly\n",
    "        from min to max values of the features (each bin has the same width).\n",
    "        - If 'quantile', they are distributed uniformly along the quantiles of\n",
    "        the features (each bin has the same number of observations).\n",
    "        - If an array-like is given, it directly specifies the sorted knot\n",
    "        positions including the boundary knots. Note that, internally,\n",
    "        `degree` number of knots are added before the first knot, the same\n",
    "        after the last knot.\n",
    "    extrapolation : {'error', 'constant', 'linear', 'continue'}, default='constant'\n",
    "        If 'error', values outside the min and max values of the training features\n",
    "        raises a `ValueError`. If 'constant', the value of the splines at minimum\n",
    "        and maximum value of the features is used as constant extrapolation. If\n",
    "        'linear', a linear extrapolation is used. If 'continue', the splines are\n",
    "        extrapolated as is, i.e. option `extrapolate=True` in `scipy.interpolate.BSpline`.\n",
    "    include_bias : bool, default=False\n",
    "        If False, then the last spline element inside the data range of a feature\n",
    "        is dropped. As B-splines sum to one over the spline basis functions for each\n",
    "        data point, they implicitly include a bias term.\n",
    "    order : {'C', 'F'}, default='C'\n",
    "        Order of output array. 'F' order is faster to compute, but may slow\n",
    "        down subsequent estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110d440",
   "metadata": {},
   "source": [
    "For this, we will create some synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "108f1f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 10 + (x * np.sin(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "359582ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_support = np.linspace(0, 15, 100)\n",
    "y_support = f(x_support)\n",
    "\n",
    "x_train = np.sort(np.random.choice(x_support[15:-15], size=25, replace=False))\n",
    "y_train = f(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5a2122df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(data=x_train, columns=['input'])\n",
    "X_support = pd.DataFrame(data=x_support, columns=['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbeac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):    \n",
    "    fig = plt.figure(figsize=(14, 3.5), dpi=96)\n",
    "    layout = (1, 1)\n",
    "    ax = plt.subplot2grid(layout, (0, 0))\n",
    "    \n",
    "    ax.plot(X_support, y_support, label='ground truth', c='#fc8d59')\n",
    "    ax.plot(X_train, y_train, 'o', label='training points', c='#fc8d59')\n",
    "    ax.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e56834",
   "metadata": {},
   "source": [
    "**Cubic spline without extrapolation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55950abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = SplineEncoder(feature='input', n_knots=5, degree=3, strategy=\"quantile\", \n",
    "                        extrapolation=\"constant\", include_bias=True,) \n",
    "\n",
    "model = make_pipeline(enc, LinearRegression(fit_intercept=False))\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_support)\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):    \n",
    "    fig = plt.figure(figsize=(14, 3.5), dpi=96)\n",
    "    layout = (1, 1)\n",
    "    ax = plt.subplot2grid(layout, (0, 0))\n",
    "    \n",
    "    ax.plot(X_support, y_support, label='ground truth', c='#fc8d59')\n",
    "    ax.plot(X_train, y_train, 'o', label='training points', c='#fc8d59')\n",
    "    ax.plot(X_support, pred, label='spline approximation')\n",
    "    ax.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a54ac1c",
   "metadata": {},
   "source": [
    "**With linear extrapolation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce324b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = SplineEncoder(feature='input', n_knots=5, degree=3, strategy=\"quantile\", \n",
    "                        extrapolation=\"linear\", include_bias=True,) \n",
    "\n",
    "model = make_pipeline(enc, LinearRegression(fit_intercept=False))\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_support)\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):    \n",
    "    fig = plt.figure(figsize=(14, 3.5), dpi=96)\n",
    "    layout = (1, 1)\n",
    "    ax = plt.subplot2grid(layout, (0, 0))\n",
    "    \n",
    "    ax.plot(X_support, y_support, label='ground truth', c='#fc8d59')\n",
    "    ax.plot(X_train, y_train, 'o', label='training points', c='#fc8d59')\n",
    "    ax.plot(X_support, pred, label='spline approximation')\n",
    "    ax.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644a7c0e",
   "metadata": {},
   "source": [
    "## Encoding pairwise interactions between numerical features\n",
    "\n",
    "### `eensight.features.encode.ISplineEncoder` \n",
    "\n",
    "Encode the interaction between two spline-encoded numerical features\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoder_left : eensight.features.encode.SplineEncoder\n",
    "        The encoder for the first of the two features\n",
    "    encoder_right : eensight.features.encode.SplineEncoder\n",
    "        The encoder for the second of the two features\n",
    "\n",
    "**Note**: If one or both of the encoders is already fitted, it will not be \n",
    "          re-fitted during `fit` or `fit_transform`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb08a23c",
   "metadata": {},
   "source": [
    "Generate the [â€œFriedman #1â€](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_friedman1.html) regression problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "06a3e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_friedman1(n_samples=5000, n_features=5, noise=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4459e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(data=X, columns=[f'x_{i}' for i in range(5)])\n",
    "y = pd.Series(data=y, index=X.index)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5d761588",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "90634eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_1 = SplineEncoder(feature='x_0', n_knots=5, degree=3, strategy=\"quantile\", \n",
    "                        extrapolation=\"constant\", include_bias=False,)\n",
    "enc_2 = SplineEncoder(feature='x_1', n_knots=5, degree=3, strategy=\"quantile\", \n",
    "                        extrapolation=\"constant\", include_bias=False,)\n",
    "enc_3 = SplineEncoder(feature='x_2', n_knots=5, degree=3, strategy=\"quantile\", \n",
    "                        extrapolation=\"constant\", include_bias=False,)\n",
    "enc_4 = SplineEncoder(feature='x_3', n_knots=5, degree=3, strategy=\"quantile\", \n",
    "                        extrapolation=\"constant\", include_bias=False,)\n",
    "enc_5 = SplineEncoder(feature='x_4', n_knots=5, degree=3, strategy=\"quantile\", \n",
    "                        extrapolation=\"constant\", include_bias=False,)\n",
    "\n",
    "interact = ISplineEncoder(enc_1, enc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "32730fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "                    ('inter', interact),\n",
    "                    ('enc_3', enc_3),\n",
    "                    ('enc_4', enc_4),\n",
    "                    ('enc_5', enc_5)\n",
    "    \n",
    "                ])\n",
    "    ),\n",
    "    ('regression', LinearRegression(fit_intercept=True))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "65a60d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb87966",
   "metadata": {},
   "source": [
    "The root mean squared error is very close to the noise that was injected in the data (0.2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aad52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Root mean squared out-of-sample error: ' \n",
    "      f'{mean_squared_error(np.array(y_test), pipeline.predict(X_test), squared=False)}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e258c39",
   "metadata": {},
   "source": [
    "Linear interations are also supported through `ProductEncoder`:\n",
    "\n",
    "### `eensight.features.encode.ProductEncoder` \n",
    "\n",
    "Encode the interaction between two linear numerical features\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoder_left : eensight.features.encode.IdentityEncoder\n",
    "        The encoder for the first of the two features\n",
    "    encoder_right : eensight.features.encode.IdentityEncoder\n",
    "        The encoder for the second of the two features\n",
    "\n",
    "**Note**: If one or both of the encoders is already fitted, it will not be\n",
    "        re-fitted during `fit` or `fit_transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d68153bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_1 = IdentityEncoder(feature='x_0', include_bias=False,)\n",
    "enc_2 = IdentityEncoder(feature='x_1', include_bias=False,)\n",
    "\n",
    "interact = ProductEncoder(enc_1, enc_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9748591c",
   "metadata": {},
   "source": [
    "This interaction is practically an element-wise multiplication of the two features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7dc27dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(interact.fit_transform(X).squeeze(), X[['x_0', 'x_1']].prod(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d14030",
   "metadata": {},
   "source": [
    "## Encoding pairwise interactions between categorical and numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855ee33e",
   "metadata": {},
   "source": [
    "We will use a different dataset this time: a hypothetical study of weight loss for 900 participants in a year-long study of 3 different exercise programs, a jogging program, a swimming program, and a reading program which serves as a control activity. Variables include:\n",
    "\n",
    "- **loss**: weight loss (continuous), positive = weight loss, negative scores = weight gain\n",
    "- **hours**: hours spent exercising (continuous)\n",
    "- **effort**: effort during exercise (continuous), 0 = minimal physical effort and 50 = maximum effort\n",
    "- **prog**: exercise program (categorical)\n",
    "    - jogging=1\n",
    "    - swimming=2\n",
    "    - reading=3\n",
    "- **gender**: participant gender (binary)\n",
    "    - male=1\n",
    "    - female=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c579aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'play_data/weight_loss/data.csv'\n",
    "data = pd.read_csv(path)\n",
    "data = data.drop('id', axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcfbc7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data, train_size=0.5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078aa7eb",
   "metadata": {},
   "source": [
    "We want to identify the way `prog` (categorical feature) and `hours` (numerical feature) interact: `loss ~ prog:hours`. \n",
    "\n",
    "First, we can explore the case where we split all the data by `prog` and fit a `loss ~ hours` model to each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9818611",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "for group, grouped_data in data_train.groupby('prog'):\n",
    "    model = LinearRegression(fit_intercept=True).fit(grouped_data[['hours']], \n",
    "                                                     grouped_data['loss'])\n",
    "    models[group] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17678b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = ['#74a9cf', '#fc8d59', '#9e9ac8']\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):    \n",
    "    fig = plt.figure(figsize=(14, 4.5), dpi=96)\n",
    "    layout = (1, 1)\n",
    "    ax = plt.subplot2grid(layout, (0, 0))\n",
    "    \n",
    "    resid = []\n",
    "    for i, (group, grouped_data) in enumerate(data_test.groupby('prog')):\n",
    "        pred = models[group].predict(grouped_data[['hours']])\n",
    "        resid.append(grouped_data['loss'].values - pred)\n",
    "        \n",
    "        ax.plot(grouped_data['hours'], pred, '.', label=f'prog: {group}', c=color_list[i])\n",
    "        ax.plot(grouped_data['hours'], grouped_data['loss'], 'o', c=color_list[i], alpha=0.2)\n",
    "        \n",
    "    ax.legend(loc='upper left')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d251c9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean squared error: {np.mean(np.concatenate(resid)**2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b217b8b",
   "metadata": {},
   "source": [
    "The same result can be achieved by first encoding the `prog` feature in one-hot form and then taking the tensor product between its encoding and the `hours` feature. In this case, an intercept must be added directly to the `hours` feature, so that it is possible to model a different intercept for each categorical feature's level:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98034853",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_cat = CategoricalEncoder(feature='prog', encode_as='onehot')\n",
    "feature_cat = enc_cat.fit_transform(data_train)\n",
    "feature_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5c1481",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tensor_product(feature_cat, add_constant(data_train['hours']))\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "754a3403",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True).fit(features, data_train['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75fb11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cat = enc_cat.transform(data_test)\n",
    "features = tensor_product(feature_cat, add_constant(data_test['hours']))\n",
    "pred = model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313cd7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = data_test['loss'].values - pred\n",
    "print(f'Mean squared error: {np.mean(resid**2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac72abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = ['#74a9cf', '#fc8d59', '#9e9ac8']\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):    \n",
    "    fig = plt.figure(figsize=(14, 4.5), dpi=96)\n",
    "    layout = (1, 1)\n",
    "    ax = plt.subplot2grid(layout, (0, 0))\n",
    "    \n",
    "    for i in range(enc_cat.n_features_out_):\n",
    "        mask = feature_cat[:, i]==1\n",
    "        ax.plot(data_test['hours'][mask], pred[mask], '.', label=f'prog: {i}', \n",
    "                c=color_list[i])\n",
    "        ax.plot(data_test['hours'][mask], data_test['loss'][mask], 'o', \n",
    "                c=color_list[i], alpha=0.3)\n",
    "        \n",
    "    ax.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b3313d",
   "metadata": {},
   "source": [
    "The conclusion here is that for the case of one categorical and one linear numerical feature, we can model the interaction by first encoding the categorical feature in one-hot form and then taking the tensor product between this encoding and the numerical feature. \n",
    "\n",
    "This is supported by `eensight.features.encode.ICatLinearEncoder`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265d682c",
   "metadata": {},
   "source": [
    "### `eensight.features.encode.ICatLinearEncoder` \n",
    "\n",
    "Encodes the interaction between one categorical and one linear numerical feature\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoder_cat : eensight.features.encode.CategoricalEncoder\n",
    "        The encoder for the categorical feature. It must encode features in an one-hot\n",
    "        form.\n",
    "    encoder_num : eensight.features.encode.IdentityEncoder\n",
    "        The encoder for the numerical feature. The  encoder should have \n",
    "        `include_bias=True`. This is necessary so that so that it is possible \n",
    "         to model a different intercept for each categorical feature's level.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48f66575",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_cat = CategoricalEncoder(feature='prog', encode_as='onehot')\n",
    "enc_num = IdentityEncoder(feature='hours', include_bias=True)\n",
    "\n",
    "enc = ICatLinearEncoder(encoder_cat=enc_cat, encoder_num=enc_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73c359a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = enc.fit_transform(data_train)\n",
    "model = LinearRegression(fit_intercept=False).fit(features, data_train['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c82633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = enc.transform(data_test)\n",
    "pred = model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a48f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = data_test['loss'].values - pred\n",
    "print(f'Mean squared error: {np.mean(resid**2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b802027",
   "metadata": {},
   "source": [
    "Next, we want to also encode the `hours` feature with splines so that to capture potential non-linearities (although in this specific dataset there are no non-linearities). \n",
    "\n",
    "A ***first split, then encode*** strategy looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27588476",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "encoders = {}\n",
    "\n",
    "for group, grouped_data in data_train.groupby('prog'):\n",
    "    enc = SplineEncoder(feature='hours', n_knots=3, degree=2, \n",
    "                                    strategy=\"quantile\", \n",
    "                                    extrapolation=\"constant\", \n",
    "                                    include_bias=True,) \n",
    "    features = enc.fit_transform(grouped_data)\n",
    "    model = LinearRegression(fit_intercept=False).fit(features, grouped_data['loss'])\n",
    "    models[group] = model\n",
    "    encoders[group] = enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440bfcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = ['#74a9cf', '#fc8d59', '#9e9ac8']\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):    \n",
    "    fig = plt.figure(figsize=(14, 4.5), dpi=96)\n",
    "    layout = (1, 1)\n",
    "    ax = plt.subplot2grid(layout, (0, 0))\n",
    "    \n",
    "    resid = []\n",
    "    for i, (group, grouped_data) in enumerate(data_test.groupby('prog')):\n",
    "        features = encoders[group].transform(grouped_data)\n",
    "        pred = models[group].predict(features)\n",
    "        resid.append(grouped_data['loss'].values - pred)\n",
    "        \n",
    "        ax.plot(grouped_data['hours'], pred, '.', label=f'prog: {group}', c=color_list[i])\n",
    "        ax.plot(grouped_data['hours'], grouped_data['loss'], 'o', c=color_list[i], alpha=0.2)\n",
    "        \n",
    "    ax.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a6faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean squared error: {np.mean(np.concatenate(resid)**2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2280cc91",
   "metadata": {},
   "source": [
    "The `eensight` implements the **first split, then encode** strategy in `ICatSplineEncoder`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e816e246",
   "metadata": {},
   "source": [
    "### `eensight.features.encode.ICatSplineEncoder` \n",
    "\n",
    "Encodes the interaction between one categorical and one spline-encoded numerical\n",
    "feature. This encoder can also work with a cyclical numerical feature.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoder_cat : eensight.features.encode.CategoricalEncoder\n",
    "        The encoder for the categorical feature. It must encode features in\n",
    "        an one-hot form.\n",
    "    encoder_num : eensight.features.encode.SplineEncoder or\n",
    "        eensight.features.encode.CyclicalEncoder\n",
    "        The encoder for the numerical feature.\n",
    "\n",
    "**Notes**: \n",
    "\n",
    "- If the categorical encoder is already fitted, it will not be re-fitted during `fit` or `fit_transform`. \n",
    "\n",
    "- The numerical encoder will always be fitted (one encoder per level of categorical feature).\n",
    "\n",
    "- If the numerical encoder is a `SplineEncoder`, it should have `include_bias=True`. This is necessary so that so that it is possible to model a different intercept for each categorical feature's level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d9aaf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_cat = CategoricalEncoder(feature='prog', encode_as='onehot')\n",
    "\n",
    "enc_num = SplineEncoder(feature='hours', n_knots=3, degree=2, \n",
    "                                    strategy=\"quantile\", \n",
    "                                    extrapolation=\"constant\", \n",
    "                                    include_bias=True,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c12f7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = ICatSplineEncoder(encoder_cat=enc_cat, encoder_num=enc_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a87901b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = enc.fit_transform(data_train)\n",
    "model = LinearRegression(fit_intercept=False).fit(features, data_train['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "555a26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = enc.transform(data_test)\n",
    "pred = model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65425856",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = data_test['loss'].values - pred\n",
    "print(f'Mean squared error: {np.mean(resid**2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc3ab2a",
   "metadata": {},
   "source": [
    "We can further explore the **first split, then encode** strategy by predicting: `loss ~ gender:prog:hours`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f058b820",
   "metadata": {},
   "source": [
    "The ***first split, then encode*** strategy is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27debd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "encoders = {}\n",
    "\n",
    "for gender, level_1_data in data_train.groupby('gender'):\n",
    "    for prog, grouped_data in level_1_data.groupby('prog'):\n",
    "        enc = SplineEncoder(feature='hours', n_knots=3, degree=2, \n",
    "                                    strategy=\"quantile\", \n",
    "                                    extrapolation=\"constant\", \n",
    "                                    include_bias=True,)\n",
    "        features = enc.fit_transform(grouped_data)\n",
    "        encoders[(gender, prog)] = enc\n",
    "        \n",
    "        model = LinearRegression(fit_intercept=False).fit(features, grouped_data['loss'])\n",
    "        models[(gender, prog)] = model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb59edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = None\n",
    "for gender, level_1_data in data_test.groupby('gender'):\n",
    "    for prog, grouped_data in level_1_data.groupby('prog'):\n",
    "        features = encoders[(gender, prog)].transform(grouped_data)\n",
    "        pred = pd.concat((pred, pd.Series(models[(gender, prog)].predict(features), \n",
    "                                          index=grouped_data.index)))\n",
    "        \n",
    "pred = pred.reindex(data_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f482eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = data_test['loss'].values - pred\n",
    "print(f'Mean squared error: {np.mean(resid**2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db14bb1",
   "metadata": {},
   "source": [
    "This is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a3e9684",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_cat = CategoricalEncoder(feature='prog', encode_as='onehot')\n",
    "enc_cat = enc_cat.fit(data_train)\n",
    "\n",
    "enc_num = SplineEncoder(feature='hours', n_knots=3, degree=2, strategy=\"quantile\", \n",
    "                        extrapolation=\"constant\", include_bias=True,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e5de1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "encoders = {}\n",
    "\n",
    "for gender, grouped_data in data_train.groupby('gender'):\n",
    "    enc = ICatSplineEncoder(encoder_cat=enc_cat, encoder_num=enc_num)\n",
    "    features = enc.fit_transform(grouped_data)\n",
    "    \n",
    "    model = LinearRegression(fit_intercept=False).fit(features, grouped_data['loss'])\n",
    "    models[gender] = model\n",
    "    encoders[gender] = enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34f80531",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = None\n",
    "for gender, grouped_data in data_test.groupby('gender'):\n",
    "    features = encoders[gender].transform(grouped_data)\n",
    "    pred = pd.concat((pred, pd.Series(models[gender].predict(features), \n",
    "                                      index=grouped_data.index)))\n",
    "        \n",
    "pred = pred.reindex(data_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88da93fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = data_test['loss'].values - pred\n",
    "print(f'Mean squared error: {np.mean(resid**2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a20d2cd",
   "metadata": {},
   "source": [
    "Despite its name, the `ICatSplineEncoder` endoder can work also when the numerical feature is encoded with `CyclicalFeatures`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad53087",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(index=pd.date_range(start='1/1/2018', end='31/12/2019', freq='D'))\n",
    "data['weekday'] = data.index.dayofweek < 5\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b9760272",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_cat = CategoricalEncoder(feature='weekday', encode_as='onehot')\n",
    "enc_cat = enc_cat.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d60814cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_num = CyclicalFeatures(period=365, fourier_order=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4f948238",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = ICatSplineEncoder(encoder_cat=enc_cat, encoder_num=enc_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db65ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = enc.fit_transform(data)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc5e9f9",
   "metadata": {},
   "source": [
    "For the case of cyclical data, the **first split then encode** and the **first encode then split** strategies are equivalent, because the encoding uses only the info of each row and not any other value from the same column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0a7f7847",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_num = CyclicalFeatures(period=365, fourier_order=3).fit_transform(data)\n",
    "features_cat = CategoricalEncoder(feature='weekday', encode_as='onehot').fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf113f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(features, tensor_product(features_cat, features_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d54d48",
   "metadata": {},
   "source": [
    "Combining a `CategoricalEncoder` with a `CyclicalFeatures` is one way to create features of custom seasonalities very similarly to how the [Prophet](https://github.com/facebook/prophet) python library does it: https://facebook.github.io/prophet/docs/seasonality,_holiday_effects,_and_regressors.html#specifying-custom-seasonalities "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff33efb6",
   "metadata": {},
   "source": [
    "------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
