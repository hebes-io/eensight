{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9533d1e8",
   "metadata": {},
   "source": [
    "# The configuration approach\n",
    "\n",
    "The configuration functionality of `eensight` builds on top of the configuration functionality provided by [`Kedro`](https://github.com/quantumblacklabs/kedro). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d416cea0",
   "metadata": {},
   "source": [
    "One of the changes that `eensight` has introduced to the `Kedro` approach is a custom `ConfigLoader` (`eensight.config.OmegaConfigLoader`) that utilizes [`OmegaConf`](https://github.com/omry/omegaconf) as the backend. This makes it easy to use [variable interpolation](https://omegaconf.readthedocs.io/en/latest/usage.html#variable-interpolation) when writting the configuration files. As an example, the `globals.yaml` file contains values that can be reused in other files:  \n",
    "\n",
    "*globals.yaml:*\n",
    "```erlang\n",
    "data_root : data\n",
    "\n",
    "types:\n",
    "  csv      : pandas.CSVDataSet\n",
    "  multiple : PartitionedDataSet\n",
    "  pickle   : pickle.PickleDataSet\n",
    "\n",
    "folders:\n",
    "  raw          : 01_raw\n",
    "  intermediate : 02_intermediate\n",
    "  primary      : 03_primary\n",
    "  feature      : 04_feature\n",
    "  model_input  : 05_model_input\n",
    "  model        : 06_models\n",
    "  model_output : 07_model_output\n",
    "  report       : 08_reporting\n",
    "```\n",
    "\n",
    "\n",
    "... and then *some_catalog.yaml:*\n",
    "```erlang\n",
    "site_name : demo\n",
    "\n",
    "sources:\n",
    "  train.root_input:\n",
    "    type: ${globals:types.multiple}\n",
    "    path: ${globals:data_root}/${site_name}/${globals:folders.raw}/train\n",
    "    dataset:\n",
    "      type: ${globals:types.csv}\n",
    "      load_args:\n",
    "        sep: ','\n",
    "        index_col: 0\n",
    "    filename_suffix: '.csv'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96dad9d",
   "metadata": {},
   "source": [
    "There are four (4) types of configuration files in `eensight`:\n",
    "\n",
    "- Data catalogs\n",
    "- Model configurations\n",
    "- Parameter values\n",
    "- Run command arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0e750a",
   "metadata": {},
   "source": [
    "## Data catalogs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c99e27d",
   "metadata": {},
   "source": [
    "Data catalogs are a way to describe the different datasets that are consumed and/or be produced by the `eensight` pipelines. A data catalog includes: \n",
    "\n",
    "- The name of the building/site\n",
    "\n",
    "- Its location (this information is used for automatically generating holiday information),  \n",
    "\n",
    "- A mapping between specific feature names that the `eensight` functionality expects (consumption, temperature, holiday, timestamp) and the actual names used in the catalog's datasets\n",
    "\n",
    "- The different data sources consumed and produced by `eensight`. The information that is necessary to adequately describe a data source can be found at Kedro's documentation: https://kedro.readthedocs.io/en/stable/05_data/01_data_catalog.html    \n",
    "\n",
    "`eensight` supports three namespaces: `train`, `test` and `post`. \n",
    "\n",
    "`train` concerns the pre-intervention data and the pipelines that are used for building, optimizing, cross-validating and adding uncertainty information to models that predict baseline energy consumption. \n",
    "\n",
    "`test` concerns pre-intervention data that the baseline model did not see during its fitting and optimization, and pipelines that evaluate the baseline model on this data. \n",
    "\n",
    "`post` concerns post-intervention data, and pipelines that calculate cumulative energy savings and their uncertainty intervals.\n",
    "\n",
    "Adding the appropriate namespace to a dataset's name helps automate the process of selecting and running pipelines.\n",
    "\n",
    "```erlang\n",
    "train.preprocessed_data:\n",
    "    type: ${globals:types.csv}\n",
    "    filepath: ${globals:data_root}/${site_name}/${globals:folders.intermediate}/train/preprocessed.csv\n",
    "    load_args:\n",
    "      sep: ','\n",
    "      index_col : 0\n",
    "      parse_dates : \n",
    "        - 0\n",
    "    save_args:\n",
    "      index: true\n",
    "    versioned : ${versioned}\n",
    "\n",
    "  test.preprocessed_data:\n",
    "    type: ${globals:types.csv}\n",
    "    filepath: ${globals:data_root}/${site_name}/${globals:folders.intermediate}/test/preprocessed.csv\n",
    "    load_args:\n",
    "      sep: ','\n",
    "      index_col : 0\n",
    "      parse_dates :\n",
    "        - 0\n",
    "    save_args:\n",
    "      index: true\n",
    "    versioned : ${versioned}\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b61c162",
   "metadata": {},
   "source": [
    "### Load the data catalog for the demo building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ab6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb8b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eensight.utils.jupyter import load_catalog\n",
    "from eensight.pipelines.preprocessing import apply_rebind_names, merge_data_list, validate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633075ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = load_catalog('demo')\n",
    "catalog.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10f6c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = catalog.load('train.root_input')\n",
    "print(type(train_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bef63d2",
   "metadata": {},
   "source": [
    "`train_input` is a partitioned dataset, so we want to validate and merge at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981fbc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "rebind_names = catalog.load('rebind_names')\n",
    "\n",
    "for key, load_func in train_input.items():\n",
    "    df = load_func()\n",
    "    df = apply_rebind_names(df, rebind_names)\n",
    "    df = validate_data(df)\n",
    "    datasets.append(df)\n",
    "    \n",
    "merged_data = merge_data_list(datasets, primary='consumption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206ec829",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = merged_data['consumption'].isna()\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):    \n",
    "    fig = plt.figure(figsize=(12, 3.54), dpi=96)\n",
    "    layout = (1, 1)\n",
    "    ax = plt.subplot2grid(layout, (0, 0))\n",
    "\n",
    "    merged_data.loc[~missing, 'consumption'].plot(ax=ax, alpha=0.5)\n",
    "    ax.set_xlabel('Hours')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a30b5a4",
   "metadata": {},
   "source": [
    "## Model configurations\n",
    "\n",
    "`eensight` is built around ensembles of localized linear regreesion models. The structure of these base models (main effects and pairwise interactions) can be defined using YAML files: \n",
    "\n",
    "```yaml\n",
    "regressors:\n",
    "  month:           # the name of the regressor\n",
    "    feature: month # the name of the feature to use and encode so that to create the regressor \n",
    "    type: categorical\n",
    "    max_n_categories: null\n",
    "    encode_as: onehot \n",
    "    interaction_only: false\n",
    "  \n",
    "  tow:\n",
    "    feature: hourofweek\n",
    "    type: categorical\n",
    "    max_n_categories: null \n",
    "    stratify_by: null \n",
    "    excluded_categories: null \n",
    "    unknown_value: null\n",
    "    min_samples_leaf: null  \n",
    "    max_features: null \n",
    "    encode_as: onehot \n",
    "    interaction_only: false\n",
    "  \n",
    "  temperature:\n",
    "    feature: temperature\n",
    "    type: linear\n",
    "    include_bias: false\n",
    "\n",
    "interactions:\n",
    "  tow, temperature:\n",
    "    tow:\n",
    "      max_n_categories: 5\n",
    "      stratify_by: temperature \n",
    "      min_samples_leaf: 20\n",
    "    temperature:\n",
    "      type: spline\n",
    "      n_knots: 4\n",
    "      degree: 2\n",
    "      strategy: quantile\n",
    "      extrapolation: constant\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8174387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = load_catalog('demo', model='towt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9426d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.load('models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28642913",
   "metadata": {},
   "source": [
    "## Parameter values\n",
    "\n",
    "`eensight` pipelines get their parameter settings for YAML files in the conf/base/parameters directory.\n",
    "\n",
    "```\n",
    "conf\n",
    "│   README.md \n",
    "│\n",
    "└───base\n",
    "│   │   globals.yaml\n",
    "│   │   logging.yaml\n",
    "│   │\n",
    "│   └── parameters\n",
    "│       └── default\n",
    "│           │   preprocess.yaml\n",
    "│           │   ...\n",
    "```\n",
    "\n",
    "Parameters are accessed and treated exactly as prescribed by the Kedro documentation: https://kedro.readthedocs.io/en/stable/04_kedro_project_setup/02_configuration.html#parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b03682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = load_catalog('demo', model='towt', parameters='default')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4e054",
   "metadata": {},
   "source": [
    "## Run command arguments\n",
    "\n",
    "The primary way of using `eensight` is through the command line:\n",
    "\n",
    "<code>python -m eensight </code>\n",
    "\n",
    "The command line functionality uses the [Hydra framework](https://hydra.cc/), which can dynamically create a hierarchical configuration by composition and override it through config files and the command line.\n",
    "\n",
    "```python\n",
    "from .settings import DEFAULT_CATALOG, DEFAULT_MODEL, DEFAULT_PARAMETERS, PROJECT_PATH\n",
    "\n",
    "@hydra.main(config_path=\"hydra\", config_name=\"run_config\")\n",
    "def run(cfg: DictConfig):\n",
    "    cfg = OmegaConf.to_container(cfg)\n",
    "\n",
    "    runner = cfg.get(\"runner\") or \"SequentialRunner\"\n",
    "    runner_class = load_obj(runner, \"kedro.runner\")\n",
    "\n",
    "    parameters = cfg.get(\"parameter_config\") or DEFAULT_PARAMETERS\n",
    "    catalog = cfg.get(\"catalog_config\") or DEFAULT_CATALOG\n",
    "    model = cfg.get(\"model_config\") or DEFAULT_MODEL\n",
    "    ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f54649b",
   "metadata": {},
   "source": [
    "The `run_config.yaml` file includes all the availabe command line options:\n",
    "\n",
    "```yaml\n",
    "# The name of the catalog configuration file to load\n",
    "catalog : null \n",
    "# The name of the model configuration file to load\n",
    "model : null\n",
    "# The name of the parameters configuration file to load\n",
    "parameters : null \n",
    "#Name of the modular pipeline to run. If not set, the project pipeline is run \n",
    "# by default\n",
    "pipeline : null \n",
    "# Specify a runner that you want to run the pipeline with.\n",
    "# Available runners: `SequentialRunner`, `ParallelRunner` and `ThreadRunner`\n",
    "# If not set, `SequentialRunner` will be used.\n",
    "runner : null \n",
    "# Load and save node inputs and outputs asynchronously with threads. \n",
    "# If not specified, load and save datasets synchronously\n",
    "async : False\n",
    "# Kedro configuration environment name. Defaults to `local`.\n",
    "env: null \n",
    "# A list of dataset names which should be used as a starting point\n",
    "from_inputs : null \n",
    "# A list of dataset names which should be used as an end point\n",
    "to_outputs : null \n",
    "# A list of node names which should be used as a starting point\n",
    "from_nodes : null \n",
    "# A list of node names which should be used as an end point\n",
    "to_nodes : null \n",
    "# A list with node names. The `run` function will run only the nodes with \n",
    "# specified names\n",
    "nodes : null \n",
    "# List of tags. Construct a pipeline from nodes having any of these tags\n",
    "tags : null\n",
    "# A mapping between dataset names and versions to load. Has no effect on \n",
    "# data sets without enabled versioning. \n",
    "load_versions : null\n",
    "# Specify extra parameters that you want to pass to the context initializer. \n",
    "# The value of these parameters will override the values in the `parameters`\n",
    "# configuration file\n",
    "params : null \n",
    "```\n",
    "\n",
    "`eensight` uses its onw `CustomContext` (that extends `KedroContext`) so that the selected catalog, model and parameter configuration files are passed to the `OmegaConfigLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d719b8a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
