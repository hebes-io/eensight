{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f210af",
   "metadata": {},
   "source": [
    "# Constructing uncertainty intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e7d5cc",
   "metadata": {},
   "source": [
    "Suppose we have a training dataset $(X_i,Y_i) \\in R^d×R$, $i=1,..,n$, and a new test point $(X_{n+1},Y_{n+1})$ drawn from the same distribution. If we have a regression model $\\hat{\\mu}$  that has been fitted on the training data, we can apply it on the new test point’s features to get a prediction for the target $\\hat{Y}_{n+1}=\\hat{\\mu}(X_{n+1})$. \n",
    "\n",
    "In addition, we want a prediction interval for the test point, i.e. an interval around $\\hat{\\mu}(X_{n+1})$  that is likely to contain the true value of $Y_{n+1}$. If the desired miscoverage rate is $a$ (or alternatively, the desired confidence level for the interval is $1-a$), this goal can be stated as find:\n",
    "\n",
    "$$C_a(X_{n+1}):  P\\{Y_{n+1} \\in C_a(X_{n+1} ) \\} \\geq 1-a$$\n",
    "\n",
    "\n",
    "`eensight` uses [conformal prediction](https://arxiv.org/abs/2107.07511) for distribution-free uncertainty quantification. \n",
    "\n",
    "The method builds upon the cross-validation functionality. If `CrossValidator` is instantiated with `keep_estimators=True`, we get: (a) a set of estimators, each of which has been fitted on a subset of the data, and (b) the index of the data subset that each estimator did not see during fitting. \n",
    "\n",
    "More formally, during cross-validation, we can split the training dataset into $K$ subsets $S_1,…,S_K$, and define $\\hat{\\mu}_{/S_k}$ as a regression model that has been fitted onto the training data with the $k$-th subset removed. `CrossValidator` stores each model $\\hat{\\mu}_{/S_k}$ (in attribute `estimators`) alongside with a mapping of the form $S_k(i) \\rightarrow \\hat{\\mu}_{/S_k}$ (in attribute `oos_masks`, here *oos* stands for out-of-sample), where $S_k(i)$ identifies the subset that contains the observation $X_i$.\n",
    "\n",
    "Conformal estimation uses the non-conformity scores defined as:\n",
    "\n",
    "<img src=\"images/conformal_01.png?modified=12345678\" alt=\"grouped\" width=\"500\"/>\n",
    "\n",
    "Then, the conformal prediction interval is defined as:\n",
    "\n",
    "<img src=\"images/conformal_02.png?modified=1234567\" alt=\"grouped\" width=\"700\"/>\n",
    "\n",
    "where $q_{n,a}^+ (e_i)$ is the $\\lceil(1-a)(n+1)\\rceil$-th smallest value of $e_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1b597c",
   "metadata": {},
   "source": [
    "The conformal prediction functionality of `eensight` resides in:\n",
    "\n",
    "- `eensight.pipelines.uncertainty.IcpEstimator`\n",
    "- `eensight.pipelines.uncertainty.AggregatedCp`\n",
    "\n",
    "\n",
    "### `eensight.pipelines.uncertainty.IcpEstimator`\n",
    "\n",
    "Inductive conformal estimator.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : Any regressor with scikit-learn predictor API (i.e. with fit and\n",
    "        predict methods)\n",
    "        The object to use to calculate the non-conformity scores. The estimator is\n",
    "        expected to be the result of a cross-validation process and it must be already\n",
    "        fitted.\n",
    "    oos_mask : array-like\n",
    "        The index of the training dataset's subset that the `estimator` has not seen\n",
    "        during its fitting (i.e. the test sample of the relevant cross-validation fold).\n",
    "    add_normalizer : bool, default=True\n",
    "        If True, a normalization model will be added. Its predictions act as a\n",
    "        multiplicative correction factor of the non-conformity scores. The\n",
    "        normalization model is a random forest regressor\n",
    "        (`sklearn.ensemble.RandomForestRegressor`).\n",
    "    extra_regressors : str or list of str, default=None\n",
    "        The names of the additional regressors to use for the normalization model. By\n",
    "        default, the normalization model uses only the month of year, day of week, and\n",
    "        hour of day features.\n",
    "    n_estimators : int, default=100\n",
    "        The number of trees in the normalization model.\n",
    "    min_samples_leaf : int or float, default=0.05\n",
    "        The minimum number of samples required to be at a leaf node of the\n",
    "        normalization model. A split point at any depth will only be considered\n",
    "        if it leaves at least ``min_samples_leaf`` training samples in each of\n",
    "        the left and right branches.  This may have the effect of smoothing the\n",
    "        model, especially in regression.\n",
    "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
    "        - If float, then `min_samples_leaf` is a fraction and\n",
    "        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
    "        number of samples for each node.\n",
    "    max_samples : int or float, default=0.6\n",
    "        The number of samples to draw from X to train each base estimator in the\n",
    "        normalization model.\n",
    "        - If None (default), then draw `X.shape[0]` samples.\n",
    "        - If int, then draw `max_samples` samples.\n",
    "        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
    "        `max_samples` should be in the interval `(0, 1)`.\n",
    "\n",
    "\n",
    "\n",
    "### `eensight.pipelines.uncertainty.AggregatedCp`\n",
    "\n",
    "Aggregated conformal estimator. Combines multiple IcpRegressor estimators\n",
    "    into an aggregated model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimators : List of estimators with scikit-learn predictor API (with fit\n",
    "        and predict methods).\n",
    "        Each estimator is expected to be the result of a cross-validation process\n",
    "        and it must be already fitted.\n",
    "    oos_masks : list of array-like\n",
    "        List containing the index of the training dataset's subset that each `estimator`\n",
    "        in `estimators` has not seen during its fitting (i.e. the test sample of the\n",
    "        relevant cross-validation fold).\n",
    "    add_normalizer : bool, default=True\n",
    "        If True, a normalization model will be added. Its predictions act as a\n",
    "        multiplicative correction factor of the non-conformity scores. The\n",
    "        normalization model is a random forest regressor\n",
    "        (`sklearn.ensemble.RandomForestRegressor`).\n",
    "    extra_regressors : str or list of str, default=None\n",
    "        The names of the additional regressors to use for the normalization model. By\n",
    "        default, the normalization model uses only the month of year, day of week, and\n",
    "        hour of day features.\n",
    "    n_estimators : int, default=100\n",
    "        The number of trees in the normalization model.\n",
    "    min_samples_leaf : int or float, default=0.05\n",
    "        The minimum number of samples required to be at a leaf node of the\n",
    "        normalization model. A split point at any depth will only be considered\n",
    "        if it leaves at least ``min_samples_leaf`` training samples in each of\n",
    "        the left and right branches.  This may have the effect of smoothing the\n",
    "        model, especially in regression.\n",
    "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
    "        - If float, then `min_samples_leaf` is a fraction and\n",
    "        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
    "        number of samples for each node.\n",
    "    max_samples : int or float, default=0.8\n",
    "        The number of samples to draw from X to train each base estimator in the\n",
    "        normalization model.\n",
    "        - If None (default), then draw `X.shape[0]` samples.\n",
    "        - If int, then draw `max_samples` samples.\n",
    "        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
    "        `max_samples` should be in the interval `(0, 1)`.\n",
    "    n_jobs : int, default=None\n",
    "        Number of jobs to run in parallel. ``None`` means 1 unless in a\n",
    "        `joblib.parallel_backend` context. ``-1`` means using all processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22a289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142f0bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e3aabd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eensight.utils.jupyter import load_catalog\n",
    "from eensight.pipelines.uncertainty import AggregatedCp, mpiw, picp, generate_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6ffc736",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = load_catalog('demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42832ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = catalog.load('train.cross_validator_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79501753",
   "metadata": {},
   "source": [
    "First, create a conformal predictor <ins>without a normalizer</ins>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dab8280",
   "metadata": {},
   "outputs": [],
   "source": [
    "conformal = AggregatedCp(estimators=cv_model.estimators, \n",
    "                         oos_masks=cv_model.oos_masks,\n",
    "                         add_normalizer=False,\n",
    "                         n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419004b6",
   "metadata": {},
   "source": [
    "The conformal predictor must be fitted on the **same dataset** that was used for cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15faa456",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = catalog.load('train.preprocessed_data')\n",
    "data_train = data_train.dropna()\n",
    "\n",
    "X_train = data_train.loc[~data_train['consumption_outlier'], ['temperature']]\n",
    "y_train = data_train.loc[X_train.index, ['consumption']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091cc705",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "conformal = conformal.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3e85d4",
   "metadata": {},
   "source": [
    "The `predict` function of `AggregatedCp` can be applied on any dataset for which we want uncertainty intervals, and it requires a `significance` parameter:\n",
    "\n",
    "    significance : float or list of floats between 0 and 1\n",
    "        Significance level (maximum allowed error rate) of predictions. If ``None``,\n",
    "        then intervals for all significance levels (0.01, 0.02, ..., 0.99) will be\n",
    "        computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65877321",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pred_conf = conformal.predict(data_train[['temperature']], significance=[0.9, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e032fe1",
   "metadata": {},
   "source": [
    "The result of the `predict` method is a `sklearn.utils.Bunch` (dict-like) object with fields:\n",
    "\n",
    " - *significance*: The significance levels used in the calculations, list of float.\n",
    " - *quantiles*: The quantiles of the non-conformity scores, array of shape (len(X), len(significance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c432ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prediction = cv_model.predict(data_train[['temperature']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62ec9156",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 1\n",
    "\n",
    "intervals = pd.concat((\n",
    "(prediction['consumption'] + pd.Series(data=pred_conf.quantiles[:, s], \n",
    "                                       index=prediction.index)).to_frame('consumption_high'),\n",
    " (prediction['consumption'] - pd.Series(data=pred_conf.quantiles[:, s], \n",
    "                                        index=prediction.index)).to_frame('consumption_low')\n",
    "), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4a5c91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_intervals(X, intervals, size=2000, title=''):\n",
    "    start = np.random.randint(0, high=len(intervals)-size)\n",
    "    \n",
    "    with plt.style.context('seaborn-whitegrid'):    \n",
    "        fig = plt.figure(figsize=(12, 3.54), dpi=96)\n",
    "        layout = (1, 1)\n",
    "        ax = plt.subplot2grid(layout, (0, 0))\n",
    "        \n",
    "        ax.fill_between(intervals.index[start:start+size], \n",
    "                        intervals['consumption_low'][start:start+size],\n",
    "                        intervals['consumption_high'][start:start+size],\n",
    "                        color='#DDA0DD', alpha=0.5\n",
    "        )\n",
    "        X.loc[intervals.index][start:start+size].plot(\n",
    "            ax=ax, alpha=0.8, rot=0)\n",
    "        ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2178ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_intervals(data_train[['consumption']], intervals, \n",
    "               title=f'Intervals at {pred.significance[s]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a59f0a",
   "metadata": {},
   "source": [
    "`eensight` includes two metrics for evaluating the uncertainty intervals:\n",
    "\n",
    "- Prediction Interval Coverage Probability (PICP). Computes the fraction of samples for\n",
    "    which the ground truth lies within predicted interval.\n",
    "- Mean Prediction Interval Width (MPIW). Computes the average width of the the prediction\n",
    "    intervals. Measures the sharpness of intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e1b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "picp(data_train['consumption'], intervals['consumption_low'], intervals['consumption_high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5a5b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpiw(intervals['consumption_low'], intervals['consumption_high'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19100c5",
   "metadata": {},
   "source": [
    "Next, create a conformal predictor <ins>with a normalizer</ins>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7cb05fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conformal = AggregatedCp(estimators=cv_model.estimators, \n",
    "                         oos_masks=cv_model.oos_masks,\n",
    "                         add_normalizer=True,\n",
    "                         max_samples=0.8,\n",
    "                         n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d409321",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "conformal = conformal.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a8f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pred_conf = conformal.predict(data_train[['temperature']], significance=[0.9, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da64b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 1\n",
    "\n",
    "intervals = pd.concat((\n",
    "(prediction['consumption'] + pd.Series(data=pred_conf.quantiles[:, s], \n",
    "                                       index=prediction.index)).to_frame('consumption_high'),\n",
    " (prediction['consumption'] - pd.Series(data=pred_conf.quantiles[:, s], \n",
    "                                        index=prediction.index)).to_frame('consumption_low')\n",
    "), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e909fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_intervals(data_train[['consumption']], intervals, \n",
    "               title=f'Intervals at {pred.significance[s]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406050b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "picp(data_train['consumption'], intervals['consumption_low'], intervals['consumption_high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02850bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpiw(intervals['consumption_low'], intervals['consumption_high'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f628d2a",
   "metadata": {},
   "source": [
    "## Sampling from the uncertainty intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db059783",
   "metadata": {},
   "source": [
    "Uncertainty intervals are useful for evaluating the confidence one should have about a model's predictions, but in M&V we need to quantify the uncertainty of cumulative sums of the predictions (more accurately, cumulative sums of the predicted minus the actual consumption).\n",
    "\n",
    "For this purpose, `eensight` includes functionality for sampling from the distribution that is implied by the uncertainty intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd6208",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "samples = generate_samples(500, prediction=prediction['consumption'], \n",
    "                                significance=pred_conf.significance, \n",
    "                                quantiles=pred_conf.quantiles\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ba9b4072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(X, samples, size=2000, title=''):\n",
    "    start = np.random.randint(0, high=len(X)-size)\n",
    "    \n",
    "    with plt.style.context('seaborn-whitegrid'):    \n",
    "        fig = plt.figure(figsize=(12, 3.54), dpi=96)\n",
    "        layout = (1, 1)\n",
    "        ax = plt.subplot2grid(layout, (0, 0))\n",
    "        \n",
    "        samples.iloc[start:start+size].plot(ax=ax, alpha=0.005, color='#DDA0DD', legend=False)\n",
    "            \n",
    "        X.iloc[start:start+size].plot(\n",
    "            ax=ax, alpha=0.8, rot=0)\n",
    "        ax.set_title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f88ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(data_train[['consumption']], samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2253b124",
   "metadata": {},
   "source": [
    "The larger the number of significance levels, the more accurately the sampling process will reflect the underying distribution of the predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e050f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pred_conf = conformal.predict(data_train[['temperature']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31649d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_conf.significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "samples = generate_samples(500, prediction=prediction['consumption'], \n",
    "                                significance=pred_conf.significance, \n",
    "                                quantiles=pred_conf.quantiles\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4aac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(data_train[['consumption']], samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421ebe0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
