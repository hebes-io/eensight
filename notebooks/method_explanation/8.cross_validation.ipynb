{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26a8b0d",
   "metadata": {},
   "source": [
    "# The cross-validation approach\n",
    "\n",
    "The cross-validation process that is implemented in `eensight` is governed by four (4) parameters:\n",
    "\n",
    "1. `group_by`: This parameter defines what constitutes an indivisible group of data. If its value is \"day\", the-cross validation process will consider the different days in the dataset as groups. If its value is \"week\", the different weeks in the dataset will be regarded as groups. In either case, the same group **will not** appear in two different folds. This is summarized schematically in the diagram below.\n",
    "\n",
    "   <img src=\"images/group_by.png?modified=12345678\" alt=\"grouped\" width=\"600\"/>\n",
    "\n",
    "\n",
    "2. `stratify_by`: This parameter defines how the cross-validation process will stratify the folds. The default value is \"month\", which means that the folds will preserve the percentage of month occurrences across test sets. In other words, all the test sets will contain observations from all the months that can be found in the full dataset. Alternative values for `stratify_by` are \"week\" and None (for no stratification).\n",
    "\n",
    "   This is summarized schematically in the diagram below.\n",
    "\n",
    "   <img src=\"images/stratify_by.png?modified=12345678\" alt=\"grouped\" width=\"600\"/>\n",
    "\n",
    "3. `n_splits`: The number of folds. It must be at least 2. \n",
    "\n",
    "\n",
    "4. `n_repeats`: This parameter defines the number of times the cross-validation process should be repeated. \n",
    "\n",
    "   If we choose the number of folds to be five (5), we actually choose to estimate the modelâ€™s performance on unseen data as the mean of five (5) values. This is generally a low number of values and leads to a high variance of the result. The solution, however, is not to increase the number of folds, since this would decrease the size of the test sets and the extent to which they adequately represent the data. Instead, we repeat the cross-validation process and merge the results. \n",
    "   \n",
    "   If we stratify over months, group by weeks and select the number of folds to be four (4), this roughly translates into test datasets that include one (1) week from each month. Given that each month has four (4) weeks, the possible number of week combinations that define a test set is very high. As a result, it is highly unlikely that repeating the cross-validation process a few times will lead to evaluating identical folds. Empirically four (4) repetitions of a cross-validation process with four (4) folds, i.e. 16 folds in total, is enough to provide stable performance estimations.\n",
    "\n",
    "The combination of `group_by` and `stratify_by` allows the model to see data that covers all seasonality and operating mode patterns, while still making it non trivial for it to predict on unseen data.  \n",
    "\n",
    "**Note**: We should not stratify the cross-validation folds if the model that we want to evaluate aims at forecasting. However, M&V models aim at interpolating the available data, and this makes stratification a suitable strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd75135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600edadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88c31482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eensight.utils.jupyter import load_catalog\n",
    "from eensight.pipelines.model_selection import create_splits, CrossValidator\n",
    "from eensight.pipelines.model_selection import cvrmse, nmbe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f45ce97",
   "metadata": {},
   "source": [
    "### Load some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06cd9995",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = load_catalog('demo', partial_catalog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1587572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = catalog.load('train.model_input_data')\n",
    "\n",
    "consumption_daily = data['consumption'].groupby(lambda x: x.date).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c4334c",
   "metadata": {},
   "source": [
    "When we group days or weeks, we don't want to treat same days or weeks of the year that are in different years as the same group. So, we always assign data to groups that make this distinction:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69b1ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_groups(X, group_block):\n",
    "    if group_block == \"day\":\n",
    "        grouped = X.groupby([lambda x: x.year, lambda x: x.dayofyear])\n",
    "    elif group_block == \"week\":\n",
    "        grouped = X.groupby([lambda x: x.year, lambda x: x.isocalendar()[1]])\n",
    "    elif group_block == \"month\":\n",
    "        grouped = X.groupby([lambda x: x.year, lambda x: x.month])\n",
    "    else:\n",
    "        raise ValueError(\"`groups` can be either `day`, `week` or `month`.\")\n",
    "\n",
    "    groups = None\n",
    "    for i, (_, group) in enumerate(grouped):\n",
    "        groups = pd.concat([groups, pd.Series(i, index=group.index)])\n",
    "    return groups.reindex(X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac380363",
   "metadata": {},
   "source": [
    "### Case: `group_by` is \"day\", no stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cf36aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = create_splits(data, group_by='day', stratify_by=None, \n",
    "                       n_splits=3, n_repeats=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_as_days = create_groups(data, 'day')\n",
    "\n",
    "for train_idx, test_idx in splits(data):\n",
    "    days_in_train = np.unique(data_as_days[train_idx])\n",
    "    days_in_test = np.unique(data_as_days[test_idx])\n",
    "    common_days = np.intersect1d(days_in_train, days_in_test)\n",
    "    print('Common days between train and test splits: '\n",
    "          f'{len(common_days) / data_as_days.nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db99bc66",
   "metadata": {},
   "source": [
    "We can visualize the data per split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0a4102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_splits(n_splits, splits, data, figsize=(12, 8)):\n",
    "    colors = ['#d8b365', '#01665e']\n",
    "\n",
    "    with plt.style.context('seaborn-whitegrid'):    \n",
    "        fig = plt.figure(figsize=figsize, dpi=96)\n",
    "        layout = (n_splits, 1)\n",
    "        \n",
    "        axes = []\n",
    "        for i in range(n_splits):\n",
    "            axes.append(plt.subplot2grid(layout, (i, 0)))\n",
    "        \n",
    "        for i, (train_idx, test_idx) in enumerate(splits(data)):\n",
    "            subset = consumption_daily[\n",
    "                        np.isin(consumption_daily.index, data.iloc[train_idx].index.date)\n",
    "            ]\n",
    "\n",
    "            subset.plot(ax=axes[i], color=colors[0], style='.', ms=8, alpha=0.8, \n",
    "                        label=f'split_{i}:train')\n",
    "\n",
    "            subset = consumption_daily[\n",
    "                        np.isin(consumption_daily.index, data.iloc[test_idx].index.date)\n",
    "            ]\n",
    "\n",
    "            subset.plot(ax=axes[i], color=colors[1], style='.', ms=8, alpha=0.8, \n",
    "                        label=f'split_{i}:test')\n",
    "\n",
    "        for ax in axes:\n",
    "            ax.legend(frameon=True, shadow=True, bbox_to_anchor=(1.01, 1.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0c5866",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_splits(3, splits, data, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3316e9",
   "metadata": {},
   "source": [
    "### Case: `group_by` is \"week\", no stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec95066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = create_splits(data, group_by='week', stratify_by=None, \n",
    "                       n_splits=3, n_repeats=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce9ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_as_weeks = create_groups(data, 'week')\n",
    "\n",
    "for train_idx, test_idx in splits(data):\n",
    "    weeks_in_train = np.unique(data_as_weeks[train_idx])\n",
    "    weeks_in_test = np.unique(data_as_weeks[test_idx])\n",
    "    common_weeks = np.intersect1d(weeks_in_train, weeks_in_test)\n",
    "    print('Common weeks between train and test splits: '\n",
    "          f'{len(common_weeks) / data_as_weeks.nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f5026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_splits(3, splits, data, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fcaa21",
   "metadata": {},
   "source": [
    "### Case: `group_by` is \"day\", `stratify_by` is \"week\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9b7718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_as_days = create_groups(data, 'day').to_frame('data_as_days')\n",
    "data_as_weeks = create_groups(data, 'week').to_frame('data_as_weeks')\n",
    "data_as_groups = pd.concat((data_as_days, data_as_weeks), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642e0b5f",
   "metadata": {},
   "source": [
    "Different values of `n_splits` lead to different coverage levels. In other words, the more splits we want to make, the more probable becomes that some weeks will not be represented in both train and test splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fe2d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_splits in range(2, 10):\n",
    "    splits = create_splits(data, group_by='day', stratify_by='week', \n",
    "                           n_splits=n_splits, n_repeats=1)\n",
    "    \n",
    "    for train_idx, test_idx in splits(data):\n",
    "        all_values = []\n",
    "        weeks_in_train = np.unique(data_as_groups.iloc[train_idx]['data_as_weeks'])\n",
    "        weeks_in_test = np.unique(data_as_groups.iloc[test_idx]['data_as_weeks'])\n",
    "        common_weeks = np.intersect1d(weeks_in_train, weeks_in_test)\n",
    "        all_values.append(len(common_weeks) / data_as_groups[\"data_as_weeks\"].nunique())\n",
    "    print(f'n_splits: {n_splits}, average coverage: {np.mean(all_values)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cea412",
   "metadata": {},
   "source": [
    "The grouping however is always maintained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45850491",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_splits in range(2, 10):\n",
    "    splits = create_splits(data, group_by='day', stratify_by='week', \n",
    "                           n_splits=n_splits, n_repeats=1)\n",
    "    \n",
    "    for train_idx, test_idx in splits(data):\n",
    "        all_values = []\n",
    "        days_in_train = np.unique(data_as_groups.iloc[train_idx]['data_as_days'])\n",
    "        days_in_test = np.unique(data_as_groups.iloc[test_idx]['data_as_days'])\n",
    "        common_days = np.intersect1d(days_in_train, days_in_test)\n",
    "        all_values.append(len(common_days) / data_as_groups[\"data_as_days\"].nunique())\n",
    "    print(f'n_splits: {n_splits}, common days between train and test splits: '\n",
    "          f'{np.mean(all_values)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e7b205",
   "metadata": {},
   "source": [
    "We can visualize the data per split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f250a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = create_splits(data, group_by='day', stratify_by='week', \n",
    "                        n_splits=3, n_repeats=1)\n",
    "    \n",
    "visualize_splits(3, splits, data, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29370682",
   "metadata": {},
   "source": [
    "### Case: `group_by` is \"week\", `stratify_by` is \"month\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c80294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_as_weeks = create_groups(data, 'week').to_frame('data_as_weeks')\n",
    "data_as_months = create_groups(data, 'month').to_frame('data_as_months')\n",
    "data_as_groups = pd.concat((data_as_weeks, data_as_months), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c95f0e",
   "metadata": {},
   "source": [
    "Different values of `n_splits` lead to different coverage levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf206aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_splits in range(2, 10):\n",
    "    splits = create_splits(data, group_by='week', stratify_by='month', \n",
    "                           n_splits=n_splits, n_repeats=1)\n",
    "\n",
    "    for train_idx, test_idx in splits(data):\n",
    "        all_values = []\n",
    "        months_in_train = np.unique(data_as_groups.iloc[train_idx]['data_as_months'])\n",
    "        months_in_test = np.unique(data_as_groups.iloc[test_idx]['data_as_months'])\n",
    "        common_months = np.intersect1d(months_in_train, months_in_test)\n",
    "        all_values.append(len(common_months) / data_as_groups[\"data_as_months\"].nunique())\n",
    "    print(f'n_splits: {n_splits}, average coverage: {np.mean(all_values)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fa31f1",
   "metadata": {},
   "source": [
    "The grouping however is always maintained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986cddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_splits in range(2, 10):\n",
    "    splits = create_splits(data, group_by='week', stratify_by='month', \n",
    "                           n_splits=n_splits, n_repeats=1)\n",
    "    for train_idx, test_idx in splits(data):\n",
    "        all_values = []\n",
    "        weeks_in_train = np.unique(data_as_groups.iloc[train_idx]['data_as_weeks'])\n",
    "        weeks_in_test = np.unique(data_as_groups.iloc[test_idx]['data_as_weeks'])\n",
    "        common_weeks = np.intersect1d(weeks_in_train, weeks_in_test)\n",
    "        all_values.append(len(common_weeks) / data_as_groups[\"data_as_weeks\"].nunique())\n",
    "    print(f'n_splits: {n_splits}, common weeks between train and test splits: '\n",
    "          f'{np.mean(all_values)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f588ffa1",
   "metadata": {},
   "source": [
    "We can visualize the data per split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a99c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = create_splits(data, group_by='week', stratify_by='month', \n",
    "                       n_splits=3, n_repeats=1)\n",
    "visualize_splits(3, splits, data, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5731f251",
   "metadata": {},
   "source": [
    "### Case: `group_by` is \"day\", `stratify_by` is \"month\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52daed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_as_days = create_groups(data, 'day').to_frame('data_as_days')\n",
    "data_as_months = create_groups(data, 'month').to_frame('data_as_months')\n",
    "data_as_groups = pd.concat((data_as_days, data_as_months), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be88f10",
   "metadata": {},
   "source": [
    "Since we have a sufficient number of unique days per moth, different values of `n_splits` do not affect coverage levels (unless we ask for a very high number of splits):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead8a1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_splits in range(2, 10):\n",
    "    splits = create_splits(data, group_by='day', stratify_by='month', \n",
    "                           n_splits=n_splits, n_repeats=1)\n",
    "    \n",
    "    for train_idx, test_idx in splits(data):\n",
    "        all_values = []\n",
    "        months_in_train = np.unique(data_as_groups.iloc[train_idx]['data_as_months'])\n",
    "        months_in_test = np.unique(data_as_groups.iloc[test_idx]['data_as_months'])\n",
    "        common_months = np.intersect1d(months_in_train, months_in_test)\n",
    "        all_values.append(len(common_months) / data_as_groups[\"data_as_months\"].nunique())\n",
    "    print(f'n_splits: {n_splits}, average coverage: {np.mean(all_values)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a452934",
   "metadata": {},
   "source": [
    ".. and also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ee092",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_splits in range(2, 10):\n",
    "    splits = create_splits(data, group_by='day', stratify_by='month', \n",
    "                           n_splits=n_splits, n_repeats=1)\n",
    "    for train_idx, test_idx in splits(data):\n",
    "        all_values = []\n",
    "        days_in_train = np.unique(data_as_groups.iloc[train_idx]['data_as_days'])\n",
    "        days_in_test = np.unique(data_as_groups.iloc[test_idx]['data_as_days'])\n",
    "        common_days = np.intersect1d(days_in_train, days_in_test)\n",
    "        all_values.append(len(common_days) / data_as_groups[\"data_as_days\"].nunique())\n",
    "    print(f'n_splits: {n_splits}, common days between train and test splits: '\n",
    "          f'{np.mean(all_values)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2712d374",
   "metadata": {},
   "source": [
    "We can visualize the data per split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db345caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = create_splits(data, group_by='day', stratify_by='month', \n",
    "                           n_splits=3, n_repeats=1)\n",
    "visualize_splits(3, splits, data, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2764cc",
   "metadata": {},
   "source": [
    "### Case: `group_by` is \"week\", `stratify_by` is \"month\", `n_repeats`>0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7354b17d",
   "metadata": {},
   "source": [
    "The combination `group_by='week'` and `stratify_by='month'` is the default in `eensight`.\n",
    "\n",
    "The default value for `n_splits` is 3. If one wants to apply the train/test cycle to more splits, it is advisable to increase the `n_repeats` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3cdd17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_as_weeks = create_groups(data, 'week').to_frame('data_as_weeks')\n",
    "data_as_months = create_groups(data, 'month').to_frame('data_as_months')\n",
    "data_as_groups = pd.concat((data_as_weeks, data_as_months), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a117c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_repeats in range(1, 10):\n",
    "    splits = create_splits(data, group_by='week', stratify_by='month', \n",
    "                           n_splits=3, n_repeats=n_repeats)\n",
    "    for i, (train_idx, test_idx) in enumerate(splits(data)):\n",
    "        all_values = []\n",
    "        months_in_train = np.unique(data_as_groups.iloc[train_idx]['data_as_months'])\n",
    "        months_in_test = np.unique(data_as_groups.iloc[test_idx]['data_as_months'])\n",
    "        common_months = np.intersect1d(months_in_train, months_in_test)\n",
    "        all_values.append(len(common_months) / data_as_groups[\"data_as_months\"].nunique())\n",
    "    print(f'n_repeats: {n_repeats}, total splits: {i+1}, average coverage: '\n",
    "          f'{np.mean(all_values)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142ff0dc",
   "metadata": {},
   "source": [
    "The grouping requirement is still maintained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_repeats in range(1, 10):\n",
    "    splits = create_splits(data, group_by='week', stratify_by='month', \n",
    "                           n_splits=3, n_repeats=n_repeats)\n",
    "    for i, (train_idx, test_idx) in enumerate(splits(data)):\n",
    "        all_values = []\n",
    "        weeks_in_train = np.unique(data_as_groups.iloc[train_idx]['data_as_weeks'])\n",
    "        weeks_in_test = np.unique(data_as_groups.iloc[test_idx]['data_as_weeks'])\n",
    "        common_weeks = np.intersect1d(weeks_in_train, weeks_in_test)\n",
    "        all_values.append(len(common_weeks) / data_as_groups[\"data_as_weeks\"].nunique())\n",
    "    print(f'n_repeats: {n_repeats}, total splits: {i+1}, common weeks between train and '\n",
    "          f'test splits: {np.mean(all_values)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3364f700",
   "metadata": {},
   "source": [
    "### Apply cross-validation\n",
    "\n",
    "`eensight.pipelines.model_selection.CrossValidator`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : Any regressor with scikit-learn API (i.e. with fit and predict methods)\n",
    "        The object to use to fit the data and evaluate the metrics.\n",
    "    group_by : str {None, 'day', 'week'}, default='week'\n",
    "        Parameter that defines what constitutes an indivisible group of data. The same\n",
    "        group will not appear in two different folds. If `group_by='week'`, the cross\n",
    "        validation process will consider the different weeks of the year as groups. If\n",
    "        `group_by='day'`, the different days of the year will be considered as groups.\n",
    "        If None, no groups will be considered.\n",
    "    stratify_by : str {None, 'week', 'month'}, default='month'\n",
    "        Parameter that defines if the cross validation process will stratify the folds.\n",
    "        If `stratify_by='month'`, the folds will preserve the percentage of month\n",
    "        occurrences across test sets.\n",
    "    n_splits : int, default=3\n",
    "        Number of folds. Must be at least 2.\n",
    "    n_repeats : int (default=None)\n",
    "        Number of times the cross-validation process needs to be repeated.\n",
    "    target_name : str, default='consumption'\n",
    "            It is expected that both y and the predictions of the `estimator` are\n",
    "            dataframes with a single column, the name of which is the one provided\n",
    "            for `target_name`.\n",
    "    scorers : dict, default=None\n",
    "            dict mapping scorer name to a callable. The callable object\n",
    "            should have signature ``scorer(y_true, y_pred)``.\n",
    "            The default value is:\n",
    "            `OrderedDict(\n",
    "                {\n",
    "                    \"CVRMSE\": lambda y_true, y_pred:\n",
    "                        eensight.pipelines.model_selection.cvrmse(\n",
    "                            y_true[target_name], y_pred[target_name]\n",
    "                        ),\n",
    "                    \"NMBE\": lambda y_true, y_pred:\n",
    "                        eensight.pipelines.model_selection.nmbe(\n",
    "                            y_true[target_name], y_pred[target_name]\n",
    "                        )\n",
    "                }\n",
    "            )`\n",
    "    keep_estimators : bool, default=False\n",
    "        Whether to keep the fitted estimators per fold.\n",
    "    n_jobs : int, default=None\n",
    "        Number of jobs to run in parallel. Training the estimator and computing\n",
    "        the score are parallelized over the cross-validation splits.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors.\n",
    "    verbose : int, default=0\n",
    "        The verbosity level.\n",
    "    fit_params : dict, default=None\n",
    "        Parameters to pass to the fit method of the estimator.\n",
    "    pre_dispatch : int or str, default='2*n_jobs'\n",
    "        Controls the number of jobs that get dispatched during parallel\n",
    "        execution. Reducing this number can be useful to avoid an\n",
    "        explosion of memory consumption when more jobs get dispatched\n",
    "        than CPUs can process. This parameter can be:\n",
    "            - None, in which case all the jobs are immediately\n",
    "            created and spawned. Use this for lightweight and\n",
    "            fast-running jobs, to avoid delays due to on-demand\n",
    "            spawning of the jobs\n",
    "            - An int, giving the exact number of total jobs that are\n",
    "            spawned\n",
    "            - A str, giving an expression as a function of n_jobs,\n",
    "            as in '2*n_jobs'\n",
    "    random_state : int, RandomState instance or None (default=None)\n",
    "        Controls the randomness of each repeated cross-validation instance.\n",
    "        Pass an int for reproducible output across multiple function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f43e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = catalog.load('ensemble_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5708ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = catalog.load('train.model_input_data')\n",
    "\n",
    "X_train = data_train.loc[~data_train['consumption_outlier'], ['temperature']]\n",
    "y_train = data_train.loc[X_train.index, ['consumption']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3bbca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(model, group_by='day',\n",
    "                    n_repeats=3, keep_estimators=True, \n",
    "                    n_jobs=-1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eb6731",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cv = cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad97aeec",
   "metadata": {},
   "source": [
    "The `CrossValidator` contains the cross-validation scores: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e9d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d450e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean out-of-sample CVRMSE (%): {np.mean(cv.scores_[\"CVRMSE\"])*100}')\n",
    "print(f'Mean out-of-sample NMBE: (%) {np.mean(cv.scores_[\"NMBE\"])*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf65086",
   "metadata": {},
   "source": [
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad849ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
