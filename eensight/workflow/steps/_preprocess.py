# -*- coding: utf-8 -*-

# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

import os 
import sys 
import logging

import numpy as np 
import pandas as pd 

from typing import Any, Dict
from datetime import datetime
from omegaconf.dictconfig import DictConfig

from eensight.logger import MLFlowLogger
from eensight.io._data_catalog import DataCatalog 
from eensight.workflow.steps._base import WorkflowStep 
from eensight.prediction.linear_models import seasonal_predict
from eensight.preprocessing._data_validation import validate_data, check_column_values_not_null
from eensight.preprocessing._outlier_detection import (global_filter, global_outlier_detect, 
                                                        local_outlier_detect)
from eensight.preprocessing._nan_imputation import linear_impute, iterative_impute



logging.basicConfig(stream=sys.stdout, 
                    level=logging.INFO, 
                    format='%(asctime)s:%(levelname)-8s: %(message)s'
)
logger = logging.getLogger('preprocess-step')


class PreprocessStep(WorkflowStep):
    """``PreprocessStep`` implements the data preprocessing step of the workflow

    Parameters
    __________
    catalog: eensight.io.DataCatalog
        The data catalog of the program. The data catalog acts as a single point 
        of reference, relaying load and save functions to the underlying data sets.
    parameters: omegaconf.dictconfig.DictConfig
        A configuration dictionary that provides values to the parameters of the
        functions called by this step. 
    name: str (default=None)
        Meaningful name for this step, should be something that is distinguishable and 
        understandable for debugging, logging and any other similar purposes.
    requires: dict (default=None)
        A dictionary that associates a dataset that is required input for this steps's 
        `execute` method with a dataset name in the program's data catalog
    provides: dict (default=None)
        A dictionary that associates a dataset that is generated by this step with a 
        dataset name in the program's data catalog
    ml_stage : str (default=None)
        The type of the produced data: `train`, `val` or `test`.
    rebind: A dict of key/value pairs used to redefine column names for the datasets that 
        are inputs to this steps's ``execute`` method.
    run_id : str (default=None)
        The active MLflow run's ID that this step runs under. If it is None, no tracking will 
        take place. 
    tracking_uri : str (default=None)
        The address of the local or remote MLflow tracking server. It must be provided, if
        `run_id` is not None.
    experiment_name : str (default='Default')
        The name of the experiment.
    tags : dict (default=None) 
        Tags to be added to the active MLflow run.
    """
    
    default_requires = ['consumption', 'temperature', 'holidays']
    default_provides = ['merged_data']
    
    
    def __init__(self, catalog          : DataCatalog, 
                       parameters       : DictConfig, 
                       name             : str=None,
                       requires         : Dict[str, str]=None, 
                       provides         : Dict[str, str]=None,
                       ml_stage         : str=None,
                       rebind           : Dict[str, str]=None,
                       run_id           : str = None,
                       tracking_uri     : str=None,
                       experiment_name  : str = None,
                       tags             : Dict[str, Any] = None
    ):
        super().__init__(catalog=catalog, 
                         parameters=parameters,
                         name=name, 
                         requires=requires, 
                         provides=provides, 
                         ml_stage=ml_stage,
                         rebind=rebind,
                         run_id=run_id,
                         tracking_uri=tracking_uri,
                         experiment_name=experiment_name,
                         tags=tags   
        )


    def pre_execute(self):
        if self.run_id is not None:
            prefix = self.name if self.name is not None else 'prep'
            self.ml_logger = MLFlowLogger(
                                tracking_uri=self.tracking_uri,
                                experiment_name=self.experiment_name, 
                                run_id=self.run_id,
                                tags=self.tags, 
                                prefix=prefix
            )
    
    
    def _consumption_step(self, data, *, no_change_window, allow_zero, allow_negative, 
                            trend, alpha, l1_ratio, global_c, min_samples, local_c):
        col_name = self.rebind.get('consumption') or 'consumption'
        date_col_name = self.rebind.get('timestamp') or 'timestamp'
        
        data = validate_data(data, col_name=col_name, 
                                   date_col_name=date_col_name
        )
        data[col_name] = global_filter(data[col_name], no_change_window=no_change_window,
                                                       allow_zero=allow_zero, 
                                                       allow_negative=allow_negative
        )
        result = seasonal_predict(data, target_name=col_name, 
                                        trend=trend,
                                        alpha=alpha, 
                                        l1_ratio=l1_ratio,
                                        return_model=True
        )
        
        outliers_global = global_outlier_detect(result.resid, c=global_c)
        outliers_local = local_outlier_detect(result.resid, min_samples=min_samples, c=local_c)
        outliers = np.logical_and(outliers_global, outliers_local)
        data[col_name] = data[col_name].mask(outliers, other=np.nan)

        if hasattr(self, 'ml_logger'):
            self.ml_logger.log_params({
                'consumption/no_change_window' : no_change_window,
                'consumption/allow_zero'       : allow_zero, 
                'consumption/allow_negative'   : allow_negative,
                'consumption/trend'            : trend,
                'consumption/alpha'            : alpha,
                'consumption/l1_ratio'         : l1_ratio,
                'consumption/global_c'         : global_c,
                'consumption/min_samples'      : min_samples,
                'consumption/local_c'          : local_c
            })
            self.ml_logger.log_model(result.model, 'seasonal-consumption-model')

        return data


    def _temperature_step(self, data, *, no_change_window, allow_zero, allow_negative, 
                            global_c, min_samples, local_c, impute_window):
        col_name = self.rebind.get('temperature') or 'temperature'
        date_col_name = self.rebind.get('timestamp') or 'timestamp'
        
        data = validate_data(data, col_name=col_name, 
                                   date_col_name=date_col_name
        )
        data[col_name] = global_filter(data[col_name], no_change_window=no_change_window,
                                                       allow_zero=allow_zero, 
                                                       allow_negative=allow_negative
        )
        outliers_global = global_outlier_detect(data[col_name], c=global_c)
        outliers_local = local_outlier_detect(data[col_name], min_samples=min_samples, c=local_c)
        outliers = np.logical_and(outliers_global, outliers_local)
        data[col_name] = data[col_name].mask(outliers, other=np.nan)
        data[col_name] = linear_impute(data[col_name], window=impute_window)

        if hasattr(self, 'ml_logger'):
            self.ml_logger.log_params({
                'temperature/no_change_window' : no_change_window,
                'temperature/allow_zero'       : allow_zero, 
                'temperature/allow_negative'   : allow_negative,
                'temperature/global_c'         : global_c,
                'temperature/min_samples'      : min_samples,
                'temperature/local_c'          : local_c,
                'temperature/impute_window'    : impute_window
            })
        
        return data


    def _holidays_step(self, data):
        col_name = self.rebind.get('holiday') or 'holiday'
        date_col_name = self.rebind.get('timestamp') or 'timestamp'
        data = validate_data(data, col_name=col_name, date_col_name=date_col_name) 
        return data


    def execute(self):
        consumption = self.catalog.load(self.requires['consumption'])
        consumption = self._consumption_step(consumption, **self.parameters.consumption_step)

        temperature = self.catalog.load(self.requires['temperature'])
        temperature = self._temperature_step(temperature, **self.parameters.temperature_step)

        holidays = None
        if self.catalog.exists(self.requires['holidays']):
            holidays = self.catalog.load(self.requires['holidays'])
            holidays = self._holidays_step(holidays)
       
        # Create intermediate data 
        merged_data = pd.merge_asof(consumption, temperature, left_index=True, 
                                                              right_index=True,
                                                              direction='nearest', 
                                                              tolerance=pd.Timedelta('1H'))
        if holidays is not None:
            holiday_col_name = self.rebind.get('holiday') or 'holiday'
            holidays.index = holidays.index.map(lambda x: x.date)
            holidays = (merged_data.index.to_series()
                                         .map(lambda x: x.date())
                                         .map(lambda x: holidays.iloc[:,0]
                                         .get(x, default=np.nan))
                                         .to_frame(holiday_col_name)
            )
            merged_data = merged_data.join(holidays)

        # Ensure that enough training data is available
        consumption_col_name = self.rebind.get('consumption') or 'consumption'
        temperature_col_name = self.rebind.get('temperature') or 'temperature'
        missing = merged_data[[consumption_col_name]].mask(merged_data[temperature_col_name].isna(), np.nan)

        avail_data = dict()
        for month_year, group in missing.groupby([lambda x: x.year, lambda x: x.month]):
            check = check_column_values_not_null(data=group, column=consumption_col_name, mostly=0.9)
            avail_data[month_year] = check.result['unexpected_percent']

        avail_data = {f'{key[0]}M{key[1]}' :val for key, val in avail_data.items()}
        avail_data = pd.DataFrame.from_dict(avail_data, orient='index', columns=['values'])
        not_enough = avail_data.query('values>0.1')
        if len(not_enough) > 0:
            logger.warning(f'Months {not_enough.index.tolist()} do not contain enough '
                            'consumption data for constructing a baseline')

        # Impute missing values in the consumption data
        imputed = iterative_impute(merged_data, target_name=consumption_col_name, 
                                                other_features=temperature_col_name)

        merged_data['consumption_imputed'] = False
        merged_data['consumption_imputed'] = (
            merged_data['consumption_imputed'].mask(merged_data[consumption_col_name].isna(), other=True)
        )
        merged_data[consumption_col_name] = (
                merged_data[consumption_col_name].mask(merged_data['consumption_imputed'], other=imputed)
        ) 
        
        save_path = self.catalog.save(self.provides['merged_data'], merged_data)
        if hasattr(self, 'ml_logger'):
            self.ml_logger.log_artifact(save_path, os.path.join(self.ml_stage, 'merged-data'))


    def post_execute(self):
        """Code to be run after executing the step."""
        if hasattr(self, 'ml_logger'):
             self.ml_logger.experiment.set_terminated(self.run_id, status='FINISHED')
    
    
    def __call__(self):
        logger.info('Running pre_execute')
        self.pre_execute()
        logger.info('Running execute')
        self.execute()
        logger.info('Running post_execute')
        self.post_execute()